{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8a6d890f-892d-4db1-9a2a-cccd115cf738",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import pickle\n",
    "import json\n",
    "import re\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "import string\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "# Device setup\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dc0005ee-680d-4557-ac20-a2c0f2c5ebce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model trained for 5 epochs\n",
      "Vocabulary size: 50000\n"
     ]
    }
   ],
   "source": [
    "# Load model checkpoint\n",
    "checkpoint_path = '../../models/qa/best_qa_model.pt'\n",
    "checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "word2idx = checkpoint['vocab']['word2idx']\n",
    "idx2word = checkpoint['vocab']['idx2word']\n",
    "config = checkpoint['config']\n",
    "epoch_trained = checkpoint.get('epoch', None)\n",
    "dev_exact_acc = checkpoint.get('dev_exact_acc', None)\n",
    "\n",
    "print(f\"Loaded model trained for {epoch_trained + 1 if epoch_trained is not None else 'unknown'} epochs\")\n",
    "\n",
    "if dev_exact_acc is not None:\n",
    "    print(f\"Best Dev Exact Acc: {dev_exact_acc:.4f}\")\n",
    "print(f\"Vocabulary size: {config['vocab_size']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4cdcb7a7-a917-4655-b3a7-497e8d8ea63b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recreate model architecture (copy from training notebook)\n",
    "class Highway(nn.Module):\n",
    "    def __init__(self, size, num_layers=1):\n",
    "        super().__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.nonlinear = nn.ModuleList([nn.Linear(size, size) for _ in range(num_layers)])\n",
    "        self.linear = nn.ModuleList([nn.Linear(size, size) for _ in range(num_layers)])\n",
    "        self.gate = nn.ModuleList([nn.Linear(size, size) for _ in range(num_layers)])\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in range(self.num_layers):\n",
    "            gate = torch.sigmoid(self.gate[layer](x))\n",
    "            nonlinear = F.relu(self.nonlinear[layer](x))\n",
    "            linear = self.linear[layer](x)\n",
    "            x = gate * nonlinear + (1 - gate) * linear\n",
    "        return x\n",
    "\n",
    "class BiDAFModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim=300, hidden_dim=128, dropout=0.2):\n",
    "        super().__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=word2idx['<PAD>'])\n",
    "        self.embedding_dropout = nn.Dropout(dropout)\n",
    "        self.highway = Highway(embed_dim, num_layers=2)\n",
    "        self.context_lstm = nn.LSTM(embed_dim, hidden_dim, batch_first=True, bidirectional=True, dropout=dropout)\n",
    "        self.question_lstm = nn.LSTM(embed_dim, hidden_dim, batch_first=True, bidirectional=True, dropout=dropout)\n",
    "        self.att_weight_c = nn.Linear(2 * hidden_dim, 1, bias=False)\n",
    "        self.att_weight_q = nn.Linear(2 * hidden_dim, 1, bias=False)\n",
    "        self.att_weight_cq = nn.Linear(2 * hidden_dim, 1, bias=False)\n",
    "        self.modeling_lstm1 = nn.LSTM(8 * hidden_dim, hidden_dim, batch_first=True, bidirectional=True, dropout=dropout)\n",
    "        self.modeling_lstm2 = nn.LSTM(2 * hidden_dim, hidden_dim, batch_first=True, bidirectional=True, dropout=dropout)\n",
    "        self.start_linear = nn.Linear(10 * hidden_dim, 1)\n",
    "        self.end_linear = nn.Linear(10 * hidden_dim, 1)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self._init_weights()\n",
    "        \n",
    "    def _init_weights(self):\n",
    "        for name, param in self.named_parameters():\n",
    "            if 'weight' in name and len(param.shape) > 1:\n",
    "                nn.init.xavier_uniform_(param)\n",
    "            elif 'bias' in name:\n",
    "                nn.init.constant_(param, 0)\n",
    "                \n",
    "    def forward(self, context, question):\n",
    "        batch_size = context.size(0)\n",
    "        context_len = context.size(1)\n",
    "        question_len = question.size(1)\n",
    "        context_mask = (context != word2idx['<PAD>']).float()\n",
    "        question_mask = (question != word2idx['<PAD>']).float()\n",
    "        context_emb = self.embedding(context)\n",
    "        question_emb = self.embedding(question)\n",
    "        context_emb = self.highway(context_emb)\n",
    "        question_emb = self.highway(question_emb)\n",
    "        context_emb = self.embedding_dropout(context_emb)\n",
    "        question_emb = self.embedding_dropout(question_emb)\n",
    "        context_enc, _ = self.context_lstm(context_emb)\n",
    "        question_enc, _ = self.question_lstm(question_emb)\n",
    "        similarity = self._compute_similarity(context_enc, question_enc)\n",
    "        question_mask_expanded = question_mask.unsqueeze(1).expand(-1, context_len, -1)\n",
    "        similarity = similarity.masked_fill(question_mask_expanded == 0, -1e9)\n",
    "        c2q_att = F.softmax(similarity, dim=2)\n",
    "        c2q = torch.bmm(c2q_att, question_enc)\n",
    "        max_similarity = torch.max(similarity, dim=2)[0]\n",
    "        q2c_att = F.softmax(max_similarity, dim=1)\n",
    "        q2c = torch.bmm(q2c_att.unsqueeze(1), context_enc)\n",
    "        q2c = q2c.expand(-1, context_len, -1)\n",
    "        G = torch.cat([context_enc, c2q, context_enc * c2q, context_enc * q2c], dim=2)\n",
    "        G = self.dropout(G)\n",
    "        M1, _ = self.modeling_lstm1(G)\n",
    "        M2, _ = self.modeling_lstm2(M1)\n",
    "        start_input = torch.cat([G, M1], dim=2)\n",
    "        end_input = torch.cat([G, M2], dim=2)\n",
    "        start_logits = self.start_linear(start_input).squeeze(-1)\n",
    "        end_logits = self.end_linear(end_input).squeeze(-1)\n",
    "        start_logits = start_logits.masked_fill(context_mask == 0, -1e9)\n",
    "        end_logits = end_logits.masked_fill(context_mask == 0, -1e9)\n",
    "        return start_logits, end_logits\n",
    "        \n",
    "    def _compute_similarity(self, context_enc, question_enc):\n",
    "        batch_size, context_len, hidden_size = context_enc.size()\n",
    "        question_len = question_enc.size(1)\n",
    "        context_expanded = context_enc.unsqueeze(2).expand(-1, -1, question_len, -1)\n",
    "        question_expanded = question_enc.unsqueeze(1).expand(-1, context_len, -1, -1)\n",
    "        elementwise_prod = context_expanded * question_expanded\n",
    "        alpha = (self.att_weight_c(context_expanded) +\n",
    "                 self.att_weight_q(question_expanded) +\n",
    "                 self.att_weight_cq(elementwise_prod))\n",
    "        return alpha.squeeze(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a81de9da-e3c7-455b-9e69-a5c32c802b16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "# Load and prepare model\n",
    "model = BiDAFModel(\n",
    "    vocab_size=config['vocab_size'],\n",
    "    embed_dim=config['embed_dim'],\n",
    "    hidden_dim=config['hidden_dim'],\n",
    "    dropout=0.0\n",
    ").to(device)\n",
    "\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "model.eval()\n",
    "print(\"Model loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "37d0c414-ab37-4b27-a5b2-52a2c0b8be00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenization and input encoding (match training pipeline)\n",
    "def clean_text(text):\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    return text.strip()\n",
    "\n",
    "def simple_tokenize(text):\n",
    "    text = re.sub(r\"([.!?])\", r\" \\1 \", text)\n",
    "    text = re.sub(r\"[^a-zA-Z0-9.!?]+\", r\" \", text)\n",
    "    return text.split()\n",
    "\n",
    "def encode_text(text, word2idx, max_len):\n",
    "    tokens = simple_tokenize(clean_text(text.lower()))\n",
    "    ids = [word2idx.get(token, word2idx['<PAD>']) for token in tokens[:max_len]]\n",
    "    ids += [word2idx['<PAD>']] * (max_len - len(ids))\n",
    "    return ids, tokens[:max_len]\n",
    "\n",
    "def get_best_span(start_logits, end_logits, max_answer_length=30):\n",
    "    start_probs = F.softmax(start_logits, dim=0)\n",
    "    end_probs = F.softmax(end_logits, dim=0)\n",
    "    best_score = 0\n",
    "    best_start = 0\n",
    "    best_end = 0\n",
    "    for start_idx in range(len(start_probs)):\n",
    "        for end_idx in range(start_idx, min(start_idx+max_answer_length, len(end_probs))):\n",
    "            score = start_probs[start_idx] * end_probs[end_idx]\n",
    "            if score > best_score:\n",
    "                best_score = score\n",
    "                best_start = start_idx\n",
    "                best_end = end_idx\n",
    "    return best_start, best_end, float(best_score)\n",
    "\n",
    "# Main prediction function\n",
    "def predict_answer(context, question, model, word2idx, max_context_len=400, max_question_len=50, max_answer_length=30):\n",
    "    context_ids, context_tokens = encode_text(context, word2idx, max_context_len)\n",
    "    question_ids, question_tokens = encode_text(question, word2idx, max_question_len)\n",
    "    context_tensor = torch.tensor([context_ids], dtype=torch.long).to(device)\n",
    "    question_tensor = torch.tensor([question_ids], dtype=torch.long).to(device)\n",
    "    with torch.no_grad():\n",
    "        start_logits, end_logits = model(context_tensor, question_tensor)\n",
    "        start_idx, end_idx, confidence = get_best_span(start_logits[0], end_logits[0], max_answer_length)\n",
    "        if start_idx < len(context_tokens) and end_idx < len(context_tokens):\n",
    "            answer_tokens = context_tokens[start_idx:end_idx + 1]\n",
    "            answer_text = ' '.join(answer_tokens)\n",
    "        else:\n",
    "            answer_text = \"\"\n",
    "            confidence = 0.0\n",
    "    return {\n",
    "        'answer': answer_text,\n",
    "        'start_idx': start_idx,\n",
    "        'end_idx': end_idx,\n",
    "        'confidence': confidence,\n",
    "        'context_tokens': context_tokens,\n",
    "        'question_tokens': question_tokens\n",
    "    }\n",
    "\n",
    "# SQuAD-style normalization & metrics\n",
    "def normalize_answer(s):\n",
    "    def remove_articles(text):\n",
    "        regex = re.compile(r'\\b(a|an|the)\\b', re.IGNORECASE)\n",
    "        return re.sub(regex, ' ', text)\n",
    "    def white_space_fix(text):\n",
    "        return ' '.join(text.split())\n",
    "    def remove_punc(text):\n",
    "        exclude = set(string.punctuation)\n",
    "        return ''.join(ch for ch in text if ch not in exclude)\n",
    "    def lower(text):\n",
    "        return text.lower()\n",
    "    return white_space_fix(remove_articles(remove_punc(lower(s))))\n",
    "\n",
    "def get_tokens(s):\n",
    "    if not s:\n",
    "        return []\n",
    "    return normalize_answer(s).split()\n",
    "\n",
    "def compute_exact(a_gold, a_pred):\n",
    "    return int(normalize_answer(a_gold) == normalize_answer(a_pred))\n",
    "\n",
    "def compute_f1(a_gold, a_pred):\n",
    "    gold_toks = get_tokens(a_gold)\n",
    "    pred_toks = get_tokens(a_pred)\n",
    "    common = Counter(gold_toks) & Counter(pred_toks)\n",
    "    num_same = sum(common.values())\n",
    "    if len(gold_toks) == 0 or len(pred_toks) == 0:\n",
    "        return int(gold_toks == pred_toks)\n",
    "    if num_same == 0:\n",
    "        return 0\n",
    "    precision = 1.0 * num_same / len(pred_toks)\n",
    "    recall = 1.0 * num_same / len(gold_toks)\n",
    "    return (2 * precision * recall) / (precision + recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5a887b3d-2763-4f66-a1c9-ccaf579c1aea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 10570 development examples for evaluation\n"
     ]
    }
   ],
   "source": [
    "# Load dev data for evaluation\n",
    "with open('../../data/q&a/dev_processed.pkl', 'rb') as f:\n",
    "    dev_data = pickle.load(f)\n",
    "    \n",
    "print(f\"Loaded {len(dev_data)} development examples for evaluation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "227601d0-44fe-42b1-a93a-3c4e21202e0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation loop with progress bar and live metrics\n",
    "def evaluate_on_squad(model, dev_data, word2idx, num_examples=1000):\n",
    "    exact_scores, f1_scores = [], []\n",
    "    predictions = {}\n",
    "    eval_data = dev_data[:num_examples]\n",
    "    print(f\"Evaluating on {len(eval_data)} examples...\")\n",
    "    pbar = tqdm(eval_data, dynamic_ncols=True)\n",
    "    running_exact = 0\n",
    "    running_f1 = 0\n",
    "    for idx, example in enumerate(pbar):\n",
    "        # Reconstruct context and question from tokens if needed\n",
    "        context_text = ' '.join(example['context_tokens']) if 'context_tokens' in example else example['context']\n",
    "        question_text = ' '.join(example['question_tokens']) if 'question_tokens' in example else example['question']\n",
    "        ground_truth = example['answer_text']\n",
    "        result = predict_answer(context_text, question_text, model, word2idx)\n",
    "        predicted_answer = result['answer']\n",
    "        predictions[example['id']] = predicted_answer\n",
    "        exact = compute_exact(ground_truth, predicted_answer)\n",
    "        f1 = compute_f1(ground_truth, predicted_answer)\n",
    "        exact_scores.append(exact)\n",
    "        f1_scores.append(f1)\n",
    "        running_exact += exact\n",
    "        running_f1 += f1\n",
    "        avg_exact = running_exact / (idx + 1)\n",
    "        avg_f1 = running_f1 / (idx + 1)\n",
    "        pbar.set_postfix({\n",
    "            'EM': f'{avg_exact*100:.1f}%',\n",
    "            'F1': f'{avg_f1*100:.1f}%',\n",
    "            'Ans': (predicted_answer[:40] + '...' if len(predicted_answer) > 40 else predicted_answer)\n",
    "        })\n",
    "        \n",
    "    return {\n",
    "        'exact_match': np.mean(exact_scores) * 100,\n",
    "        'f1': np.mean(f1_scores) * 100,\n",
    "        'predictions': predictions,\n",
    "        'total_examples': len(eval_data)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "62fc4c45-3f96-4143-ba3b-cde93ebaaa6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating on 500 examples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████| 500/500 [18:40<00:00,  2.24s/it, EM=40.4%, F1=48.9%, Ans=tuesday]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Results on 500 examples:\n",
      "Exact Match: 40.40%\n",
      "F1 Score: 48.92%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Run comprehensive evaluation\n",
    "results = evaluate_on_squad(model, dev_data, word2idx, num_examples=500)\n",
    "\n",
    "print(f\"\\nResults on {results['total_examples']} examples:\")\n",
    "print(f\"Exact Match: {results['exact_match']:.2f}%\")\n",
    "print(f\"F1 Score: {results['f1']:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e7386778-a119-4fe0-b13f-ec3b53b2832e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example 1:\n",
      "Context: super bowl 50 was an american football game to determine the champion of the national football league nfl for the 2015 season . the american football conference afc champion denver broncos defeated th...\n",
      "Question: which nfl team represented the afc at super bowl 50 ?\n",
      "Ground Truth: 'Denver Broncos'\n",
      "Predicted: 'american football conference'\n",
      "Exact Match: 0, F1: 0.000\n",
      "\n",
      "Example 2:\n",
      "Context: super bowl 50 was an american football game to determine the champion of the national football league nfl for the 2015 season . the american football conference afc champion denver broncos defeated th...\n",
      "Question: which nfl team represented the nfc at super bowl 50 ?\n",
      "Ground Truth: 'Carolina Panthers'\n",
      "Predicted: 'panthers'\n",
      "Exact Match: 0, F1: 0.667\n",
      "\n",
      "Example 3:\n",
      "Context: super bowl 50 was an american football game to determine the champion of the national football league nfl for the 2015 season . the american football conference afc champion denver broncos defeated th...\n",
      "Question: where did super bowl 50 take place ?\n",
      "Ground Truth: 'Santa Clara, California'\n",
      "Predicted: 'levi s stadium in the san francisco bay area at santa clara california'\n",
      "Exact Match: 0, F1: 0.400\n",
      "\n",
      "Example 4:\n",
      "Context: super bowl 50 was an american football game to determine the champion of the national football league nfl for the 2015 season . the american football conference afc champion denver broncos defeated th...\n",
      "Question: which nfl team won super bowl 50 ?\n",
      "Ground Truth: 'Denver Broncos'\n",
      "Predicted: 'arabic'\n",
      "Exact Match: 0, F1: 0.000\n",
      "\n",
      "Example 5:\n",
      "Context: super bowl 50 was an american football game to determine the champion of the national football league nfl for the 2015 season . the american football conference afc champion denver broncos defeated th...\n",
      "Question: what color was used to emphasize the 50th anniversary of the super bowl ?\n",
      "Ground Truth: 'gold'\n",
      "Predicted: 'gold'\n",
      "Exact Match: 1, F1: 1.000\n",
      "\n",
      "Evaluation complete!\n"
     ]
    }
   ],
   "source": [
    "# Example predictions\n",
    "for i, example in enumerate(dev_data[:5]):\n",
    "    context_text = ' '.join(example['context_tokens']) if 'context_tokens' in example else example['context']\n",
    "    question_text = ' '.join(example['question_tokens']) if 'question_tokens' in example else example['question']\n",
    "    ground_truth = example['answer_text']\n",
    "    result = predict_answer(context_text, question_text, model, word2idx)\n",
    "    predicted_answer = result['answer']\n",
    "    exact_score = compute_exact(ground_truth, predicted_answer)\n",
    "    f1_score = compute_f1(ground_truth, predicted_answer)\n",
    "    print(f\"\\nExample {i+1}:\")\n",
    "    print(f\"Context: {context_text[:200]}...\")\n",
    "    print(f\"Question: {question_text}\")\n",
    "    print(f\"Ground Truth: '{ground_truth}'\")\n",
    "    print(f\"Predicted: '{predicted_answer}'\")\n",
    "    print(f\"Exact Match: {exact_score}, F1: {f1_score:.3f}\")\n",
    "\n",
    "print(\"\\nEvaluation complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "45cb7965-844a-4f8d-9b96-8cce0e59cf63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluation complete! Results saved to ../../models/qa/detailed_evaluation.json\n",
      "Overall Performance: EM=40.4%, F1=48.9%\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Interactive Q&A function\n",
    "def interactive_qa():\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"INTERACTIVE Q&A\")\n",
    "    print(\"=\"*50)\n",
    "    print(\"Enter 'quit' to exit\")\n",
    "    \n",
    "    while True:\n",
    "        print(\"\\n\" + \"-\"*30)\n",
    "        context = input(\"Enter context paragraph: \").strip()\n",
    "        if context.lower() == 'quit':\n",
    "            print(\"Exiting interactive Q&A.\")\n",
    "            break\n",
    "            \n",
    "        question = input(\"Enter question: \").strip()\n",
    "        if question.lower() == 'quit':\n",
    "            print(\"Exiting interactive Q&A.\")\n",
    "            break\n",
    "        \n",
    "        # Predict answer using your previously defined predict_answer()\n",
    "        result = predict_answer(context, question, model, word2idx)\n",
    "        \n",
    "        print(f\"\\nAnswer: '{result['answer']}'\")\n",
    "        print(f\"Confidence: {result['confidence']:.3f}\")\n",
    "        print(f\"Position: tokens {result['start_idx']}-{result['end_idx']}\")\n",
    "        \n",
    "        if result['confidence'] < 0.05:\n",
    "            print(\"Low confidence - answer might be unreliable\")\n",
    "\n",
    "# Save evaluation results\n",
    "eval_results_path = '../../models/qa/detailed_evaluation.json'\n",
    "with open(eval_results_path, 'w') as f:\n",
    "    json.dump({\n",
    "        'overall_results': {\n",
    "            'exact_match': results['exact_match'],\n",
    "            'f1': results['f1'],\n",
    "            'total_examples': results['total_examples']\n",
    "        },\n",
    "        'model_config': config,\n",
    "        'training_info': {\n",
    "            'epoch': checkpoint.get('epoch', None),\n",
    "            'dev_exact_acc': checkpoint.get('dev_exact_acc', None)\n",
    "        }\n",
    "    }, f, indent=2)\n",
    "\n",
    "print(f\"\\nEvaluation complete! Results saved to {eval_results_path}\")\n",
    "print(f\"Overall Performance: EM={results['exact_match']:.1f}%, F1={results['f1']:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0f68dfb-5417-4912-839d-8cc3cb775d8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "INTERACTIVE Q&A\n",
      "==================================================\n",
      "Enter 'quit' to exit\n",
      "\n",
      "------------------------------\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter context paragraph:  The old lighthouse keeper, Silas, had lived alone on the craggy isle for fifty years. His only companions were the gulls and the rhythmic crash of waves against the rocks. Every evening, he'd climb the winding staircase, his boots echoing in the stone tower, to light the lamp and send its beam sweeping across the dark sea. One stormy night, a small fishing boat was tossed by the tempest. Silas, with his keen eyes, saw the tiny vessel battling the waves. He knew they were in desperate need of guidance. He worked tirelessly, adjusting the lamp, making sure its light was strong and true. The boat, guided by the steady beam, found its way to safety. The next morning, the grateful fishermen came to thank Silas. He smiled, a rare and gentle smile, and said, \"The light is my duty. It's all I have to offer.\" And in that moment, Silas realized that his solitary life, once filled with quiet isolation, had found purpose in the simple act of shining.\n",
      "Enter question:  who is Silas?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Answer: 'old lighthouse'\n",
      "Confidence: 0.245\n",
      "Position: tokens 1-2\n",
      "\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Interactive Q&A\n",
    "interactive_qa()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80ec5525-fb58-4889-94c4-8309f219bf92",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
