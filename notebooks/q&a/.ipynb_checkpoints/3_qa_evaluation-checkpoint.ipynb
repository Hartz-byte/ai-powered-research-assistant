{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a6d890f-892d-4db1-9a2a-cccd115cf738",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import pickle\n",
    "import json\n",
    "import re\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "import string\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Device setup\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc0005ee-680d-4557-ac20-a2c0f2c5ebce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model checkpoint\n",
    "checkpoint = torch.load('../../models/qa/best_qa_model.pt', map_location=device)\n",
    "word2idx = checkpoint['vocab']['word2idx']\n",
    "idx2word = checkpoint['vocab']['idx2word']\n",
    "config = checkpoint['config']\n",
    "\n",
    "print(f\"Loaded model trained for {checkpoint['epoch']+1} epochs\")\n",
    "print(f\"Best Dev Exact Acc: {checkpoint['dev_exact_acc']:.4f}\")\n",
    "print(f\"Vocabulary size: {config['vocab_size']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cdcb7a7-a917-4655-b3a7-497e8d8ea63b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recreate model architecture (copy from training notebook)\n",
    "class Highway(nn.Module):\n",
    "    def __init__(self, size, num_layers=1):\n",
    "        super().__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.nonlinear = nn.ModuleList([nn.Linear(size, size) for _ in range(num_layers)])\n",
    "        self.linear = nn.ModuleList([nn.Linear(size, size) for _ in range(num_layers)])\n",
    "        self.gate = nn.ModuleList([nn.Linear(size, size) for _ in range(num_layers)])\n",
    "        \n",
    "    def forward(self, x):\n",
    "        for layer in range(self.num_layers):\n",
    "            gate = torch.sigmoid(self.gate[layer](x))\n",
    "            nonlinear = F.relu(self.nonlinear[layer](x))\n",
    "            linear = self.linear[layer](x)\n",
    "            x = gate * nonlinear + (1 - gate) * linear\n",
    "        return x\n",
    "\n",
    "class BiDAFModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim=300, hidden_dim=128, dropout=0.2):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.embed_dim = embed_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "        # Word embeddings\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=word2idx['<PAD>'])\n",
    "        self.embedding_dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        # Highway network\n",
    "        self.highway = Highway(embed_dim, num_layers=2)\n",
    "        \n",
    "        # Contextual encoding layers\n",
    "        self.context_lstm = nn.LSTM(embed_dim, hidden_dim, batch_first=True, \n",
    "                                   bidirectional=True, dropout=dropout if dropout > 0 else 0)\n",
    "        self.question_lstm = nn.LSTM(embed_dim, hidden_dim, batch_first=True, \n",
    "                                    bidirectional=True, dropout=dropout if dropout > 0 else 0)\n",
    "        \n",
    "        # Attention weights\n",
    "        self.att_weight_c = nn.Linear(2 * hidden_dim, 1, bias=False)\n",
    "        self.att_weight_q = nn.Linear(2 * hidden_dim, 1, bias=False)\n",
    "        self.att_weight_cq = nn.Linear(2 * hidden_dim, 1, bias=False)\n",
    "        \n",
    "        # Modeling layer\n",
    "        self.modeling_lstm1 = nn.LSTM(8 * hidden_dim, hidden_dim, batch_first=True,\n",
    "                                     bidirectional=True, dropout=dropout if dropout > 0 else 0)\n",
    "        self.modeling_lstm2 = nn.LSTM(2 * hidden_dim, hidden_dim, batch_first=True,\n",
    "                                     bidirectional=True, dropout=dropout if dropout > 0 else 0)\n",
    "        \n",
    "        # Output projections\n",
    "        self.start_linear = nn.Linear(10 * hidden_dim, 1)\n",
    "        self.end_linear = nn.Linear(10 * hidden_dim, 1)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, context, question):\n",
    "        batch_size = context.size(0)\n",
    "        context_len = context.size(1)\n",
    "        question_len = question.size(1)\n",
    "        \n",
    "        # Masks\n",
    "        context_mask = (context != word2idx['<PAD>']).float()\n",
    "        question_mask = (question != word2idx['<PAD>']).float()\n",
    "        \n",
    "        # Embeddings with highway network\n",
    "        context_emb = self.embedding(context)\n",
    "        question_emb = self.embedding(question)\n",
    "        \n",
    "        context_emb = self.highway(context_emb)\n",
    "        question_emb = self.highway(question_emb)\n",
    "        \n",
    "        context_emb = self.embedding_dropout(context_emb)\n",
    "        question_emb = self.embedding_dropout(question_emb)\n",
    "        \n",
    "        # Contextual encoding\n",
    "        context_enc, _ = self.context_lstm(context_emb)\n",
    "        question_enc, _ = self.question_lstm(question_emb)\n",
    "        \n",
    "        # Attention Flow Layer\n",
    "        similarity = self._compute_similarity(context_enc, question_enc)\n",
    "        \n",
    "        # Mask similarity scores\n",
    "        question_mask_expanded = question_mask.unsqueeze(1).expand(-1, context_len, -1)\n",
    "        similarity = similarity.masked_fill(question_mask_expanded == 0, -1e9)\n",
    "        \n",
    "        # Context-to-Question Attention\n",
    "        c2q_att = F.softmax(similarity, dim=2)\n",
    "        c2q = torch.bmm(c2q_att, question_enc)\n",
    "        \n",
    "        # Question-to-Context Attention\n",
    "        max_similarity = torch.max(similarity, dim=2)[0]\n",
    "        q2c_att = F.softmax(max_similarity, dim=1)\n",
    "        q2c = torch.bmm(q2c_att.unsqueeze(1), context_enc)\n",
    "        q2c = q2c.expand(-1, context_len, -1)\n",
    "        \n",
    "        # Query-aware context representation\n",
    "        G = torch.cat([\n",
    "            context_enc,\n",
    "            c2q,\n",
    "            context_enc * c2q,\n",
    "            context_enc * q2c\n",
    "        ], dim=2)\n",
    "        \n",
    "        G = self.dropout(G)\n",
    "        \n",
    "        # Modeling Layer\n",
    "        M1, _ = self.modeling_lstm1(G)\n",
    "        M2, _ = self.modeling_lstm2(M1)\n",
    "        \n",
    "        # Output Layer\n",
    "        start_input = torch.cat([G, M1], dim=2)\n",
    "        end_input = torch.cat([G, M2], dim=2)\n",
    "        \n",
    "        start_logits = self.start_linear(start_input).squeeze(-1)\n",
    "        end_logits = self.end_linear(end_input).squeeze(-1)\n",
    "        \n",
    "        # Apply context mask\n",
    "        start_logits = start_logits.masked_fill(context_mask == 0, -1e9)\n",
    "        end_logits = end_logits.masked_fill(context_mask == 0, -1e9)\n",
    "        \n",
    "        return start_logits, end_logits\n",
    "    \n",
    "    def _compute_similarity(self, context_enc, question_enc):\n",
    "        batch_size, context_len, hidden_size = context_enc.size()\n",
    "        question_len = question_enc.size(1)\n",
    "        \n",
    "        context_expanded = context_enc.unsqueeze(2).expand(-1, -1, question_len, -1)\n",
    "        question_expanded = question_enc.unsqueeze(1).expand(-1, context_len, -1, -1)\n",
    "        \n",
    "        elementwise_prod = context_expanded * question_expanded\n",
    "        \n",
    "        alpha = (self.att_weight_c(context_expanded) + \n",
    "                self.att_weight_q(question_expanded) + \n",
    "                self.att_weight_cq(elementwise_prod))\n",
    "        \n",
    "        return alpha.squeeze(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a81de9da-e3c7-455b-9e69-a5c32c802b16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize and load model\n",
    "model = BiDAFModel(\n",
    "    vocab_size=config['vocab_size'],\n",
    "    embed_dim=config['embed_dim'],\n",
    "    hidden_dim=config['hidden_dim'],\n",
    "    dropout=0.0\n",
    ").to(device)\n",
    "\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "model.eval()\n",
    "\n",
    "print(\"Model loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37d0c414-ab37-4b27-a5b2-52a2c0b8be00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text preprocessing functions\n",
    "def clean_text(text):\n",
    "    \"\"\"Basic text cleaning\"\"\"\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    text = text.strip()\n",
    "    return text\n",
    "\n",
    "def simple_tokenize(text):\n",
    "    \"\"\"Simple word tokenization\"\"\"\n",
    "    text = re.sub(r\"([.!?])\", r\" \\1 \", text)\n",
    "    text = re.sub(r\"[^a-zA-Z0-9.!?]+\", r\" \", text)\n",
    "    return text.split()\n",
    "\n",
    "def encode_text(text, word2idx, max_len):\n",
    "    \"\"\"Encode text to token IDs\"\"\"\n",
    "    tokens = simple_tokenize(clean_text(text.lower()))\n",
    "    ids = [word2idx.get(token, word2idx['<UNK>']) for token in tokens[:max_len]]\n",
    "    ids += [word2idx['<PAD>']] * (max_len - len(ids))\n",
    "    return ids, tokens[:max_len]\n",
    "\n",
    "# Advanced span selection\n",
    "def get_best_span(start_logits, end_logits, max_answer_length=30):\n",
    "    \"\"\"Get the best answer span using dynamic programming\"\"\"\n",
    "    start_probs = F.softmax(start_logits, dim=0)\n",
    "    end_probs = F.softmax(end_logits, dim=0)\n",
    "    \n",
    "    best_score = 0\n",
    "    best_start = 0\n",
    "    best_end = 0\n",
    "    \n",
    "    for start_idx in range(len(start_probs)):\n",
    "        for end_idx in range(start_idx, min(start_idx + max_answer_length, len(end_probs))):\n",
    "            score = start_probs[start_idx] * end_probs[end_idx]\n",
    "            if score > best_score:\n",
    "                best_score = score\n",
    "                best_start = start_idx\n",
    "                best_end = end_idx\n",
    "    \n",
    "    return best_start, best_end, float(best_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a887b3d-2763-4f66-a1c9-ccaf579c1aea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main prediction function\n",
    "def predict_answer(context, question, model, word2idx, max_context_len=400, max_question_len=50, max_answer_length=30):\n",
    "    \"\"\"Predict answer for given context and question\"\"\"\n",
    "    \n",
    "    # Encode inputs\n",
    "    context_ids, context_tokens = encode_text(context, word2idx, max_context_len)\n",
    "    question_ids, question_tokens = encode_text(question, word2idx, max_question_len)\n",
    "    \n",
    "    # Convert to tensors\n",
    "    context_tensor = torch.tensor([context_ids], dtype=torch.long).to(device)\n",
    "    question_tensor = torch.tensor([question_ids], dtype=torch.long).to(device)\n",
    "    \n",
    "    # Model inference\n",
    "    with torch.no_grad():\n",
    "        start_logits, end_logits = model(context_tensor, question_tensor)\n",
    "        \n",
    "        # Get best span\n",
    "        start_idx, end_idx, confidence = get_best_span(\n",
    "            start_logits[0], end_logits[0], max_answer_length\n",
    "        )\n",
    "        \n",
    "        # Extract answer from context tokens\n",
    "        if start_idx < len(context_tokens) and end_idx < len(context_tokens):\n",
    "            answer_tokens = context_tokens[start_idx:end_idx + 1]\n",
    "            answer_text = ' '.join(answer_tokens)\n",
    "        else:\n",
    "            answer_text = \"\"\n",
    "            confidence = 0.0\n",
    "    \n",
    "    return {\n",
    "        'answer': answer_text,\n",
    "        'start_idx': start_idx,\n",
    "        'end_idx': end_idx,\n",
    "        'confidence': confidence,\n",
    "        'context_tokens': context_tokens,\n",
    "        'question_tokens': question_tokens\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "227601d0-44fe-42b1-a93a-3c4e21202e0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation metrics (SQuAD style)\n",
    "def normalize_answer(s):\n",
    "    \"\"\"Lower text and remove punctuation, articles and extra whitespace.\"\"\"\n",
    "    def remove_articles(text):\n",
    "        regex = re.compile(r'\\b(a|an|the)\\b', re.IGNORECASE)\n",
    "        return re.sub(regex, ' ', text)\n",
    "    \n",
    "    def white_space_fix(text):\n",
    "        return ' '.join(text.split())\n",
    "    \n",
    "    def remove_punc(text):\n",
    "        exclude = set(string.punctuation)\n",
    "        return ''.join(ch for ch in text if ch not in exclude)\n",
    "    \n",
    "    def lower(text):\n",
    "        return text.lower()\n",
    "    \n",
    "    return white_space_fix(remove_articles(remove_punc(lower(s))))\n",
    "\n",
    "def get_tokens(s):\n",
    "    if not s:\n",
    "        return []\n",
    "    return normalize_answer(s).split()\n",
    "\n",
    "def compute_exact(a_gold, a_pred):\n",
    "    return int(normalize_answer(a_gold) == normalize_answer(a_pred))\n",
    "\n",
    "def compute_f1(a_gold, a_pred):\n",
    "    gold_toks = get_tokens(a_gold)\n",
    "    pred_toks = get_tokens(a_pred)\n",
    "    \n",
    "    common = Counter(gold_toks) & Counter(pred_toks)\n",
    "    num_same = sum(common.values())\n",
    "    \n",
    "    if len(gold_toks) == 0 or len(pred_toks) == 0:\n",
    "        return int(gold_toks == pred_toks)\n",
    "    \n",
    "    if num_same == 0:\n",
    "        return 0\n",
    "    \n",
    "    precision = 1.0 * num_same / len(pred_toks)\n",
    "    recall = 1.0 * num_same / len(gold_toks)\n",
    "    f1 = (2 * precision * recall) / (precision + recall)\n",
    "    \n",
    "    return f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62fc4c45-3f96-4143-ba3b-cde93ebaaa6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load development data for comprehensive evaluation\n",
    "with open('../../data/q&a/dev_processed.pkl', 'rb') as f:\n",
    "    dev_data = pickle.load(f)\n",
    "\n",
    "print(f\"Loaded {len(dev_data)} development examples for evaluation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7386778-a119-4fe0-b13f-ec3b53b2832e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprehensive evaluation function\n",
    "def evaluate_on_squad(model, dev_data, num_examples=1000):\n",
    "    \"\"\"Evaluate model on SQuAD-style data\"\"\"\n",
    "    \n",
    "    exact_scores = []\n",
    "    f1_scores = []\n",
    "    predictions = {}\n",
    "    \n",
    "    # Take a subset for evaluation\n",
    "    eval_data = dev_data[:num_examples]\n",
    "    \n",
    "    print(f\"Evaluating on {len(eval_data)} examples...\")\n",
    "    \n",
    "    for example in tqdm(eval_data):\n",
    "        # Reconstruct context and question from tokens\n",
    "        context_text = ' '.join(example['context_tokens'])\n",
    "        question_text = ' '.join(example['question_tokens'])\n",
    "        ground_truth = example['answer_text']\n",
    "        \n",
    "        # Predict answer\n",
    "        result = predict_answer(context_text, question_text, model, word2idx)\n",
    "        predicted_answer = result['answer']\n",
    "        \n",
    "        # Store prediction\n",
    "        predictions[example['id']] = predicted_answer\n",
    "        \n",
    "        # Calculate metrics\n",
    "        exact_score = compute_exact(ground_truth, predicted_answer)\n",
    "        f1_score = compute_f1(ground_truth, predicted_answer)\n",
    "        \n",
    "        exact_scores.append(exact_score)\n",
    "        f1_scores.append(f1_score)\n",
    "    \n",
    "    # Calculate averages\n",
    "    avg_exact = np.mean(exact_scores) * 100\n",
    "    avg_f1 = np.mean(f1_scores) * 100\n",
    "    \n",
    "    return {\n",
    "        'exact_match': avg_exact,\n",
    "        'f1': avg_f1,\n",
    "        'predictions': predictions,\n",
    "        'total_examples': len(eval_data)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c22acd9e-f567-4601-b1e7-238a5ae48f72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run comprehensive evaluation\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"COMPREHENSIVE EVALUATION\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "results = evaluate_on_squad(model, dev_data, num_examples=1000)\n",
    "\n",
    "print(f\"\\nResults on {results['total_examples']} examples:\")\n",
    "print(f\"Exact Match: {results['exact_match']:.2f}%\")\n",
    "print(f\"F1 Score: {results['f1']:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "421524c8-3d8d-43f5-a2c9-2f69a7eae2a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some example predictions\n",
    "for i, example in enumerate(dev_data[:5]):\n",
    "    context_text = ' '.join(example['context_tokens'])\n",
    "    question_text = ' '.join(example['question_tokens'])\n",
    "    ground_truth = example['answer_text']\n",
    "    \n",
    "    result = predict_answer(context_text, question_text, model, word2idx)\n",
    "    predicted_answer = result['answer']\n",
    "    \n",
    "    exact_score = compute_exact(ground_truth, predicted_answer)\n",
    "    f1_score = compute_f1(ground_truth, predicted_answer)\n",
    "    \n",
    "    print(f\"\\nExample {i+1}:\")\n",
    "    print(f\"Context: {context_text[:200]}...\")\n",
    "    print(f\"Question: {question_text}\")\n",
    "    print(f\"Ground Truth: '{ground_truth}'\")\n",
    "    print(f\"Predicted: '{predicted_answer}'\")\n",
    "    print(f\"Confidence: {result['confidence']:.3f}\")\n",
    "    print(f\"Exact Match: {exact_score}, F1: {f1_score:.3f}\")\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a27cce81-c225-4337-955d-ef50f2013031",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confidence analysis\n",
    "def analyze_by_confidence(model, dev_data, num_examples=500):\n",
    "    \"\"\"Analyze model performance by confidence levels\"\"\"\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for example in tqdm(dev_data[:num_examples], desc=\"Analyzing confidence\"):\n",
    "        context_text = ' '.join(example['context_tokens'])\n",
    "        question_text = ' '.join(example['question_tokens'])\n",
    "        ground_truth = example['answer_text']\n",
    "        \n",
    "        result = predict_answer(context_text, question_text, model, word2idx)\n",
    "        predicted_answer = result['answer']\n",
    "        \n",
    "        exact_score = compute_exact(ground_truth, predicted_answer)\n",
    "        f1_score = compute_f1(ground_truth, predicted_answer)\n",
    "        \n",
    "        results.append({\n",
    "            'confidence': result['confidence'],\n",
    "            'exact': exact_score,\n",
    "            'f1': f1_score,\n",
    "            'answer_length': len(predicted_answer.split())\n",
    "        })\n",
    "    \n",
    "    # Sort by confidence\n",
    "    results.sort(key=lambda x: x['confidence'], reverse=True)\n",
    "    \n",
    "    # Analyze high vs low confidence\n",
    "    high_conf = [r for r in results if r['confidence'] > 0.1]\n",
    "    low_conf = [r for r in results if r['confidence'] <= 0.1]\n",
    "    \n",
    "    print(f\"\\nConfidence Analysis:\")\n",
    "    print(f\"High confidence predictions ({len(high_conf)}): EM={np.mean([r['exact'] for r in high_conf])*100:.1f}%, F1={np.mean([r['f1'] for r in high_conf])*100:.1f}%\")\n",
    "    print(f\"Low confidence predictions ({len(low_conf)}): EM={np.mean([r['exact'] for r in low_conf])*100:.1f}%, F1={np.mean([r['f1'] for r in low_conf])*100:.1f}%\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"CONFIDENCE ANALYSIS\")\n",
    "print(\"=\"*50)\n",
    "analyze_by_confidence(model, dev_data, num_examples=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45cb7965-844a-4f8d-9b96-8cce0e59cf63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interactive Q&A function\n",
    "def interactive_qa():\n",
    "    \"\"\"Interactive Q&A interface\"\"\"\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"INTERACTIVE Q&A\")\n",
    "    print(\"=\"*50)\n",
    "    print(\"Enter 'quit' to exit\")\n",
    "    \n",
    "    while True:\n",
    "        print(\"\\n\" + \"-\"*30)\n",
    "        context = input(\"Enter context paragraph: \").strip()\n",
    "        if context.lower() == 'quit':\n",
    "            break\n",
    "            \n",
    "        question = input(\"Enter question: \").strip()\n",
    "        if question.lower() == 'quit':\n",
    "            break\n",
    "        \n",
    "        # Predict answer\n",
    "        result = predict_answer(context, question, model, word2idx)\n",
    "        \n",
    "        print(f\"\\nAnswer: '{result['answer']}'\")\n",
    "        print(f\"Confidence: {result['confidence']:.3f}\")\n",
    "        print(f\"Position: tokens {result['start_idx']}-{result['end_idx']}\")\n",
    "        \n",
    "        if result['confidence'] < 0.05:\n",
    "            print(\"Low confidence - answer might be unreliable\")\n",
    "\n",
    "# Save evaluation results\n",
    "with open('../../models/qa/detailed_evaluation.json', 'w') as f:\n",
    "    json.dump({\n",
    "        'overall_results': {\n",
    "            'exact_match': results['exact_match'],\n",
    "            'f1': results['f1'],\n",
    "            'total_examples': results['total_examples']\n",
    "        },\n",
    "        'model_config': config,\n",
    "        'training_info': {\n",
    "            'epoch': checkpoint['epoch'],\n",
    "            'dev_exact_acc': checkpoint['dev_exact_acc']\n",
    "        }\n",
    "    }, f, indent=2)\n",
    "\n",
    "print(f\"\\nEvaluation complete! Results saved to detailed_evaluation.json\")\n",
    "print(f\"Overall Performance: EM={results['exact_match']:.1f}%, F1={results['f1']:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0f68dfb-5417-4912-839d-8cc3cb775d8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interactive Q&A\n",
    "interactive_qa()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80ec5525-fb58-4889-94c4-8309f219bf92",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
