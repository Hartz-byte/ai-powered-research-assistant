{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b99eeae0-2a02-4af7-8793-d999e7089d91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "\n",
    "# Device setup\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "137bed5e-785e-4207-baf0-0aa847c53225",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path\n",
    "MODEL_DIR = '../../models/qa'\n",
    "\n",
    "# Hyperparameters\n",
    "BATCH_SIZE = 32\n",
    "LEARNING_RATE = 0.0008\n",
    "NUM_EPOCHS = 8\n",
    "EMBED_DIM = 300\n",
    "HIDDEN_DIM = 128\n",
    "DROPOUT = 0.2\n",
    "MAX_GRAD_NORM = 5.0\n",
    "WARMUP_PROPORTION = 0.1\n",
    "PATIENCE = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "85453d07-7000-48a4-963c-8cb869e439f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 87599 training examples\n",
      "Loaded 10570 development examples\n",
      "Vocabulary size: 50000\n"
     ]
    }
   ],
   "source": [
    "# Load processed data and vocabulary\n",
    "with open('../../data/q&a/vocab.pkl', 'rb') as f:\n",
    "    vocab_data = pickle.load(f)\n",
    "    word2idx = vocab_data['word2idx']\n",
    "    idx2word = vocab_data['idx2word']\n",
    "\n",
    "with open('../../data/q&a/train_processed.pkl', 'rb') as f:\n",
    "    train_data = pickle.load(f)\n",
    "\n",
    "with open('../../data/q&a/dev_processed.pkl', 'rb') as f:\n",
    "    dev_data = pickle.load(f)\n",
    "\n",
    "print(f\"Loaded {len(train_data)} training examples\")\n",
    "print(f\"Loaded {len(dev_data)} development examples\")\n",
    "print(f\"Vocabulary size: {len(word2idx)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9a5c325a-af5d-4019-b6ba-e21ea417d324",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset class\n",
    "class QADataset(Dataset):\n",
    "    def __init__(self, data, max_context_len=400, max_question_len=50):\n",
    "        # Filter out invalid examples\n",
    "        self.data = [ex for ex in data if ex['start_position'] >= 0 and ex['end_position'] >= 0]\n",
    "        self.max_context_len = max_context_len\n",
    "        self.max_question_len = max_question_len\n",
    "        print(f\"Filtered to {len(self.data)} valid examples\")\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        example = self.data[idx]\n",
    "        \n",
    "        # Ensure positions are within bounds\n",
    "        start_pos = min(example['start_position'], self.max_context_len - 1)\n",
    "        end_pos = min(example['end_position'], self.max_context_len - 1)\n",
    "        \n",
    "        return {\n",
    "            'context_ids': torch.tensor(example['context_ids'][:self.max_context_len], dtype=torch.long),\n",
    "            'question_ids': torch.tensor(example['question_ids'][:self.max_question_len], dtype=torch.long),\n",
    "            'start_position': torch.tensor(start_pos, dtype=torch.long),\n",
    "            'end_position': torch.tensor(end_pos, dtype=torch.long),\n",
    "            'id': example['id']\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2e34af3a-bf9f-47f5-b280-1c82851a1fa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BiDAF Model\n",
    "class BiDAFModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim=300, hidden_dim=128, dropout=0.2):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.embed_dim = embed_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "        # Word embeddings\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=word2idx['<PAD>'])\n",
    "        self.embedding_dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        # Highway network for embeddings\n",
    "        self.highway = Highway(embed_dim, num_layers=2)\n",
    "        \n",
    "        # Contextual encoding layers\n",
    "        self.context_lstm = nn.LSTM(embed_dim, hidden_dim, batch_first=True, \n",
    "                                   bidirectional=True, dropout=dropout if dropout > 0 else 0)\n",
    "        self.question_lstm = nn.LSTM(embed_dim, hidden_dim, batch_first=True, \n",
    "                                    bidirectional=True, dropout=dropout if dropout > 0 else 0)\n",
    "        \n",
    "        # Attention weights\n",
    "        self.att_weight_c = nn.Linear(2 * hidden_dim, 1, bias=False)\n",
    "        self.att_weight_q = nn.Linear(2 * hidden_dim, 1, bias=False)  \n",
    "        self.att_weight_cq = nn.Linear(2 * hidden_dim, 1, bias=False)\n",
    "        \n",
    "        # Modeling layer\n",
    "        self.modeling_lstm1 = nn.LSTM(8 * hidden_dim, hidden_dim, batch_first=True,\n",
    "                                     bidirectional=True, dropout=dropout if dropout > 0 else 0)\n",
    "        self.modeling_lstm2 = nn.LSTM(2 * hidden_dim, hidden_dim, batch_first=True,\n",
    "                                     bidirectional=True, dropout=dropout if dropout > 0 else 0)\n",
    "        \n",
    "        # Output projections\n",
    "        self.start_linear = nn.Linear(10 * hidden_dim, 1)\n",
    "        self.end_linear = nn.Linear(10 * hidden_dim, 1)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        # Initialize weights\n",
    "        self._init_weights()\n",
    "        \n",
    "    def _init_weights(self):\n",
    "        \"\"\"Initialize model weights\"\"\"\n",
    "        for name, param in self.named_parameters():\n",
    "            if 'weight' in name and len(param.shape) > 1:\n",
    "                nn.init.xavier_uniform_(param)\n",
    "            elif 'bias' in name:\n",
    "                nn.init.constant_(param, 0)\n",
    "    \n",
    "    def forward(self, context, question):\n",
    "        batch_size = context.size(0)\n",
    "        context_len = context.size(1)\n",
    "        question_len = question.size(1)\n",
    "        \n",
    "        # Masks\n",
    "        context_mask = (context != word2idx['<PAD>']).float()\n",
    "        question_mask = (question != word2idx['<PAD>']).float()\n",
    "        \n",
    "        # Embeddings with highway network\n",
    "        context_emb = self.embedding(context)\n",
    "        question_emb = self.embedding(question)\n",
    "        \n",
    "        context_emb = self.highway(context_emb)\n",
    "        question_emb = self.highway(question_emb)\n",
    "        \n",
    "        context_emb = self.embedding_dropout(context_emb)\n",
    "        question_emb = self.embedding_dropout(question_emb)\n",
    "        \n",
    "        # Contextual encoding\n",
    "        context_enc, _ = self.context_lstm(context_emb)  # (batch, context_len, 2*hidden)\n",
    "        question_enc, _ = self.question_lstm(question_emb)  # (batch, question_len, 2*hidden)\n",
    "        \n",
    "        # Attention Flow Layer\n",
    "        # Similarity matrix computation\n",
    "        similarity = self._compute_similarity(context_enc, question_enc)  # (batch, context_len, question_len)\n",
    "        \n",
    "        # Mask similarity scores\n",
    "        question_mask_expanded = question_mask.unsqueeze(1).expand(-1, context_len, -1)\n",
    "        similarity = similarity.masked_fill(question_mask_expanded == 0, -1e9)\n",
    "        \n",
    "        # Context-to-Question Attention\n",
    "        c2q_att = F.softmax(similarity, dim=2)  # (batch, context_len, question_len)\n",
    "        c2q = torch.bmm(c2q_att, question_enc)  # (batch, context_len, 2*hidden)\n",
    "        \n",
    "        # Question-to-Context Attention\n",
    "        max_similarity = torch.max(similarity, dim=2)[0]  # (batch, context_len)\n",
    "        q2c_att = F.softmax(max_similarity, dim=1)  # (batch, context_len)\n",
    "        q2c = torch.bmm(q2c_att.unsqueeze(1), context_enc)  # (batch, 1, 2*hidden)\n",
    "        q2c = q2c.expand(-1, context_len, -1)  # (batch, context_len, 2*hidden)\n",
    "        \n",
    "        # Query-aware context representation\n",
    "        G = torch.cat([\n",
    "            context_enc,\n",
    "            c2q, \n",
    "            context_enc * c2q,\n",
    "            context_enc * q2c\n",
    "        ], dim=2)  # (batch, context_len, 8*hidden)\n",
    "        \n",
    "        G = self.dropout(G)\n",
    "        \n",
    "        # Modeling Layer\n",
    "        M1, _ = self.modeling_lstm1(G)  # (batch, context_len, 2*hidden)\n",
    "        M2, _ = self.modeling_lstm2(M1)  # (batch, context_len, 2*hidden)\n",
    "        \n",
    "        # Output Layer\n",
    "        start_input = torch.cat([G, M1], dim=2)  # (batch, context_len, 10*hidden)\n",
    "        end_input = torch.cat([G, M2], dim=2)    # (batch, context_len, 10*hidden)\n",
    "        \n",
    "        start_logits = self.start_linear(start_input).squeeze(-1)  # (batch, context_len)\n",
    "        end_logits = self.end_linear(end_input).squeeze(-1)       # (batch, context_len)\n",
    "        \n",
    "        # Apply context mask\n",
    "        start_logits = start_logits.masked_fill(context_mask == 0, -1e9)\n",
    "        end_logits = end_logits.masked_fill(context_mask == 0, -1e9)\n",
    "        \n",
    "        return start_logits, end_logits\n",
    "    \n",
    "    def _compute_similarity(self, context_enc, question_enc):\n",
    "        \"\"\"Compute similarity matrix between context and question\"\"\"\n",
    "        batch_size, context_len, hidden_size = context_enc.size()\n",
    "        question_len = question_enc.size(1)\n",
    "        \n",
    "        # Expand tensors for element-wise operations\n",
    "        context_expanded = context_enc.unsqueeze(2).expand(-1, -1, question_len, -1)\n",
    "        question_expanded = question_enc.unsqueeze(1).expand(-1, context_len, -1, -1)\n",
    "        \n",
    "        # Element-wise product\n",
    "        elementwise_prod = context_expanded * question_expanded\n",
    "        \n",
    "        # Compute attention weights\n",
    "        alpha = (self.att_weight_c(context_expanded) + \n",
    "                self.att_weight_q(question_expanded) + \n",
    "                self.att_weight_cq(elementwise_prod))  # (batch, context_len, question_len, 1)\n",
    "        \n",
    "        return alpha.squeeze(-1)  # (batch, context_len, question_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e4d6b17f-54cd-4e70-8181-8ff226865f02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Highway Network for better gradient flow\n",
    "class Highway(nn.Module):\n",
    "    def __init__(self, size, num_layers=1):\n",
    "        super().__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.nonlinear = nn.ModuleList([nn.Linear(size, size) for _ in range(num_layers)])\n",
    "        self.linear = nn.ModuleList([nn.Linear(size, size) for _ in range(num_layers)])\n",
    "        self.gate = nn.ModuleList([nn.Linear(size, size) for _ in range(num_layers)])\n",
    "        \n",
    "    def forward(self, x):\n",
    "        for layer in range(self.num_layers):\n",
    "            gate = torch.sigmoid(self.gate[layer](x))\n",
    "            nonlinear = F.relu(self.nonlinear[layer](x))\n",
    "            linear = self.linear[layer](x)\n",
    "            x = gate * nonlinear + (1 - gate) * linear\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "21e0078d-115f-4aaa-8ef2-4a83970273fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered to 87599 valid examples\n",
      "Filtered to 10570 valid examples\n",
      "Training batches: 2738\n",
      "Development batches: 331\n"
     ]
    }
   ],
   "source": [
    "# Create data loaders\n",
    "train_dataset = QADataset(train_data, max_context_len=400, max_question_len=50)\n",
    "dev_dataset = QADataset(dev_data, max_context_len=400, max_question_len=50)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4, pin_memory=True)\n",
    "dev_loader = DataLoader(dev_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4, pin_memory=True)\n",
    "\n",
    "print(f\"Training batches: {len(train_loader)}\")\n",
    "print(f\"Development batches: {len(dev_loader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a5204fcb-f590-4b1b-bd8c-42ff56d82058",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/d/ML/Projects/ai-powered-research-assistant/assistant-gpu/lib/python3.13/site-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total parameters: 18,002,730\n",
      "Trainable parameters: 18,002,730\n"
     ]
    }
   ],
   "source": [
    "# Initialize model\n",
    "model = BiDAFModel(\n",
    "    vocab_size=len(word2idx),\n",
    "    embed_dim=EMBED_DIM,\n",
    "    hidden_dim=HIDDEN_DIM,\n",
    "    dropout=DROPOUT\n",
    ").to(device)\n",
    "\n",
    "# Count parameters\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"Total parameters: {total_params:,}\")\n",
    "print(f\"Trainable parameters: {trainable_params:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "43bdcc58-6753-46f6-aa18-130936b0e29b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_linear_schedule_with_warmup(optimizer, num_warmup_steps, num_training_steps):\n",
    "    def lr_lambda(current_step):\n",
    "        if current_step < num_warmup_steps:\n",
    "            return float(current_step) / float(max(1, num_warmup_steps))\n",
    "        return max(0.0, float(num_training_steps - current_step) / float(max(1, num_training_steps - num_warmup_steps)))\n",
    "    \n",
    "    return torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d82e7f5d-eb4e-47b4-ae5f-5570bc1d70d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizer, Scheduler and Loss function\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=0.01)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.8)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "98f088d4-3308-419d-bc73-cc58399ae3e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training and Evaluation function\n",
    "def train_epoch(model, loader, optimizer, epoch):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    total_start_correct = 0\n",
    "    total_end_correct = 0\n",
    "    total_samples = 0\n",
    "    \n",
    "    # Use tqdm with less frequent updates\n",
    "    pbar = tqdm(loader, desc=f\"Epoch {epoch+1} [Train]\", \n",
    "                dynamic_ncols=True, leave=False)\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    for batch_idx, batch in enumerate(pbar):\n",
    "        # Move to device\n",
    "        context = batch['context_ids'].to(device, non_blocking=True)\n",
    "        question = batch['question_ids'].to(device, non_blocking=True)\n",
    "        start_pos = batch['start_position'].to(device, non_blocking=True)\n",
    "        end_pos = batch['end_position'].to(device, non_blocking=True)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        start_logits, end_logits = model(context, question)\n",
    "        \n",
    "        # Loss computation\n",
    "        start_loss = criterion(start_logits, start_pos)\n",
    "        end_loss = criterion(end_logits, end_pos)\n",
    "        loss = start_loss + end_loss\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), MAX_GRAD_NORM)\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Update metrics (less frequently)\n",
    "        batch_size = start_pos.size(0)\n",
    "        total_loss += loss.item()\n",
    "        total_start_correct += (start_logits.argmax(1) == start_pos).sum().item()\n",
    "        total_end_correct += (end_logits.argmax(1) == end_pos).sum().item()\n",
    "        total_samples += batch_size\n",
    "        \n",
    "        # Update progress bar every 50 batches\n",
    "        if batch_idx % 50 == 0:\n",
    "            elapsed = time.time() - start_time\n",
    "            pbar.set_postfix({\n",
    "                'Loss': f'{total_loss/(batch_idx+1):.3f}',\n",
    "                'Start': f'{total_start_correct/total_samples:.3f}',\n",
    "                'End': f'{total_end_correct/total_samples:.3f}',\n",
    "                'Time': f'{elapsed:.0f}s'\n",
    "            })\n",
    "    \n",
    "    return (total_loss / len(loader), \n",
    "            total_start_correct / total_samples, \n",
    "            total_end_correct / total_samples)\n",
    "\n",
    "def eval_epoch(model, loader, epoch):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    total_start_correct = 0\n",
    "    total_end_correct = 0\n",
    "    total_exact_correct = 0\n",
    "    total_samples = 0\n",
    "    \n",
    "    pbar = tqdm(loader, desc=f\"Epoch {epoch+1} [Val]\", \n",
    "                dynamic_ncols=True, leave=False)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in pbar:\n",
    "            context = batch['context_ids'].to(device, non_blocking=True)\n",
    "            question = batch['question_ids'].to(device, non_blocking=True)\n",
    "            start_pos = batch['start_position'].to(device, non_blocking=True)\n",
    "            end_pos = batch['end_position'].to(device, non_blocking=True)\n",
    "            \n",
    "            start_logits, end_logits = model(context, question)\n",
    "            \n",
    "            start_loss = criterion(start_logits, start_pos)\n",
    "            end_loss = criterion(end_logits, end_pos)\n",
    "            loss = start_loss + end_loss\n",
    "            \n",
    "            # Predictions\n",
    "            start_pred = start_logits.argmax(1)\n",
    "            end_pred = end_logits.argmax(1)\n",
    "            \n",
    "            # Update metrics\n",
    "            batch_size = start_pos.size(0)\n",
    "            total_loss += loss.item()\n",
    "            total_start_correct += (start_pred == start_pos).sum().item()\n",
    "            total_end_correct += (end_pred == end_pos).sum().item()\n",
    "            total_exact_correct += ((start_pred == start_pos) & (end_pred == end_pos)).sum().item()\n",
    "            total_samples += batch_size\n",
    "    \n",
    "    return (total_loss / len(loader),\n",
    "            total_start_correct / total_samples,\n",
    "            total_end_correct / total_samples,\n",
    "            total_exact_correct / total_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a3bbc6a2-6720-41fa-be1b-737d6238f425",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting optimized training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/8 - Time: 1090.4s\n",
      "Train - Loss: 6.9092, Start: 0.1657, End: 0.1789\n",
      "Val   - Loss: 5.4398, Start: 0.3141, End: 0.3364, Exact: 0.2081\n",
      "LR: 0.000640\n",
      "New best model saved! Exact Acc: 0.2081\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 2/8 - Time: 1941.6s\n",
      "Train - Loss: 4.3080, Start: 0.4215, End: 0.4579\n",
      "Val   - Loss: 4.3923, Start: 0.4176, End: 0.4504, Exact: 0.3005\n",
      "LR: 0.000512\n",
      "New best model saved! Exact Acc: 0.3005\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 3/8 - Time: 2434.8s\n",
      "Train - Loss: 3.5259, Start: 0.5080, End: 0.5491\n",
      "Val   - Loss: 4.2393, Start: 0.4383, End: 0.4710, Exact: 0.3263\n",
      "LR: 0.000410\n",
      "New best model saved! Exact Acc: 0.3263\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 4/8 - Time: 1831.6s\n",
      "Train - Loss: 3.0387, Start: 0.5598, End: 0.6063\n",
      "Val   - Loss: 4.2883, Start: 0.4446, End: 0.4753, Exact: 0.3312\n",
      "LR: 0.000328\n",
      "New best model saved! Exact Acc: 0.3312\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 5/8 - Time: 1769.1s\n",
      "Train - Loss: 2.6235, Start: 0.6076, End: 0.6562\n",
      "Val   - Loss: 4.3498, Start: 0.4490, End: 0.4783, Exact: 0.3353\n",
      "LR: 0.000262\n",
      "New best model saved! Exact Acc: 0.3353\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 6/8 - Time: 1725.2s\n",
      "Train - Loss: 2.2640, Start: 0.6502, End: 0.7023\n",
      "Val   - Loss: 4.6125, Start: 0.4368, End: 0.4724, Exact: 0.3272\n",
      "LR: 0.000210\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'PATIENCE' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 49\u001b[39m\n\u001b[32m     47\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     48\u001b[39m     patience_count += \u001b[32m1\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m49\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mNo improvement. Patience: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpatience_count\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[43mPATIENCE\u001b[49m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     50\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m patience_count >= PATIENCE:\n\u001b[32m     51\u001b[39m         \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mEarly stopping triggered!\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'PATIENCE' is not defined"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "best_exact_acc = 0\n",
    "patience_count = 0\n",
    "\n",
    "print(\"Starting optimized training...\")\n",
    "overall_start_time = time.time()\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    epoch_start_time = time.time()\n",
    "    \n",
    "    # Training\n",
    "    train_loss, train_start_acc, train_end_acc = train_epoch(model, train_loader, optimizer, epoch)\n",
    "    \n",
    "    # Validation\n",
    "    val_loss, val_start_acc, val_end_acc, val_exact_acc = eval_epoch(model, dev_loader, epoch)\n",
    "    \n",
    "    # Scheduler step (once per epoch)\n",
    "    scheduler.step()\n",
    "    \n",
    "    epoch_time = time.time() - epoch_start_time\n",
    "    \n",
    "    print(f\"\\nEpoch {epoch+1}/{NUM_EPOCHS} - Time: {epoch_time:.1f}s\")\n",
    "    print(f\"Train - Loss: {train_loss:.4f}, Start: {train_start_acc:.4f}, End: {train_end_acc:.4f}\")\n",
    "    print(f\"Val   - Loss: {val_loss:.4f}, Start: {val_start_acc:.4f}, End: {val_end_acc:.4f}, Exact: {val_exact_acc:.4f}\")\n",
    "    print(f\"LR: {optimizer.param_groups[0]['lr']:.6f}\")\n",
    "    \n",
    "    # Save best model\n",
    "    if val_exact_acc > best_exact_acc:\n",
    "        best_exact_acc = val_exact_acc\n",
    "        patience_count = 0\n",
    "        \n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'val_exact_acc': val_exact_acc,\n",
    "            'vocab': {'word2idx': word2idx, 'idx2word': idx2word},\n",
    "            'config': {\n",
    "                'vocab_size': len(word2idx),\n",
    "                'embed_dim': EMBED_DIM,\n",
    "                'hidden_dim': HIDDEN_DIM,\n",
    "                'dropout': DROPOUT\n",
    "            }\n",
    "        }, os.path.join(MODEL_DIR, 'best_qa_model.pt'))\n",
    "        \n",
    "        print(f\"New best model saved! Exact Acc: {val_exact_acc:.4f}\")\n",
    "    else:\n",
    "        patience_count += 1\n",
    "        print(f\"No improvement. Patience: {patience_count}/{PATIENCE}\")\n",
    "        if patience_count >= PATIENCE:\n",
    "            print(\"Early stopping triggered!\")\n",
    "            break\n",
    "    \n",
    "    # Memory cleanup\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "total_time = time.time() - overall_start_time\n",
    "print(f\"\\nTraining completed in {total_time:.1f}s ({total_time/60:.1f} minutes)\")\n",
    "print(f\"Best validation exact accuracy: {best_exact_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1eb58d7-8dca-4056-98d3-17e6bf26806a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
