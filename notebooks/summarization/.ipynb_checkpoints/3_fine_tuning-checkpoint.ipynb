{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6dc7da7b-a43e-4836-b0d2-12adb897280b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/d/ML/Projects/ai-powered-research-assistant/assistant-gpu/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "import os, json, math, random, time\n",
    "import torch, torch.nn as nn, torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "from rouge_score import rouge_scorer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dc99dbb9-a8d8-4470-a428-d6b0b502d85c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Paths\n",
    "DATA_DIR = '../../data/summarization/'\n",
    "MODEL_DIR = '../../models/summarization/'\n",
    "VOCAB_PATH = os.path.join(MODEL_DIR, 'vocab.json')\n",
    "CHECKPOINT_PATH = os.path.join(MODEL_DIR, 'best_summarization_model.pt')\n",
    "\n",
    "# Device Setup\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d274a15b-8b2d-4b03-bc3f-d0fc2f3922ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparams\n",
    "MAX_ART_LEN = 400\n",
    "MAX_SUM_LEN = 50\n",
    "EMB_DIM = 256\n",
    "N_HEADS = 8\n",
    "FF_DIM = 512\n",
    "LAYERS = 4\n",
    "BATCH_SIZE = 32\n",
    "EXTRA_EPOCHS = 12\n",
    "INIT_LR = 2e-4\n",
    "WARMUP_EPOCHS = 1\n",
    "LABEL_SMOOTH = 0.1\n",
    "CLIP_NORM = 1.0\n",
    "TEACHER_P0 = 1.0\n",
    "TEACHER_DECAY = 0.9\n",
    "PATIENCE = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d1633b36-16d9-434f-bcab-90247dd2d737",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded vocab with size: 30000\n",
      "PAD token index: 0\n"
     ]
    }
   ],
   "source": [
    "# Load Vocabulary\n",
    "with open(VOCAB_PATH, 'r') as f:\n",
    "    vocab = json.load(f)\n",
    "\n",
    "PAD_TOKEN   = '<PAD>'\n",
    "UNK_TOKEN   = '<UNK>'\n",
    "START_TOKEN = '<START>'\n",
    "END_TOKEN   = '<END>'\n",
    "pad_idx = vocab[PAD_TOKEN]\n",
    "\n",
    "print(f\"Loaded vocab with size: {len(vocab)}\")\n",
    "print(f\"PAD token index: {pad_idx}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "de99ecb6-9422-487f-ae06-7a1c6da37f2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset class\n",
    "def tokenize(txt): \n",
    "    return txt.split()\n",
    "\n",
    "class SummarizationDataset(Dataset):\n",
    "    def __init__(self, df, vocab, max_article_len=400, max_summary_len=50):\n",
    "        self.articles = df['clean_article'].values\n",
    "        self.summaries = df['clean_summary'].values\n",
    "        self.vocab = vocab\n",
    "        self.max_article_len = max_article_len\n",
    "        self.max_summary_len = max_summary_len\n",
    "\n",
    "    def encode(self, text, max_len, add_specials=False):\n",
    "        tokens = tokenize(text)\n",
    "        if add_specials:\n",
    "            tokens = [START_TOKEN] + tokens[:max_len-2] + [END_TOKEN]\n",
    "        else:\n",
    "            tokens = tokens[:max_len]\n",
    "        ids = [self.vocab.get(w, self.vocab[UNK_TOKEN]) for w in tokens]\n",
    "        if len(ids) < max_len:\n",
    "            ids += [self.vocab[PAD_TOKEN]] * (max_len - len(ids))\n",
    "        return ids[:max_len]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.articles)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        src = torch.tensor(self.encode(self.articles[idx], self.max_article_len, add_specials=False), dtype=torch.long)\n",
    "        tgt = torch.tensor(self.encode(self.summaries[idx], self.max_summary_len, add_specials=True), dtype=torch.long)\n",
    "        return src, tgt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "83969dc3-4f07-41fd-8dfc-461740a7e60a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train batches: 1515 | Val batches: 268\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "train_df = pd.read_csv(os.path.join(DATA_DIR, 'processed_train_split.csv'))\n",
    "val_df = pd.read_csv(os.path.join(DATA_DIR, 'processed_val_split.csv'))\n",
    "\n",
    "train_loader = DataLoader(SummarizationDataset(train_df, vocab, MAX_ART_LEN, MAX_SUM_LEN), BATCH_SIZE, True, num_workers=4, pin_memory=True)\n",
    "val_loader = DataLoader(SummarizationDataset(val_df, vocab, MAX_ART_LEN, MAX_SUM_LEN), BATCH_SIZE, False, num_workers=4, pin_memory=True)\n",
    "\n",
    "print(f\"Train batches: {len(train_loader)} | Val batches: {len(val_loader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "afc2b900-0907-43b5-97fb-1583ddc08593",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TransformerSummarizer Model Class Definition\n",
    "class TransformerSummarizer(nn.Module):\n",
    "    def __init__(self, vocab_size, emb_dim, nhead, ff_dim, num_layers, max_article_len, max_summary_len, pad_idx):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, emb_dim, padding_idx=pad_idx)\n",
    "        self.pos_encoder = nn.Embedding(max_article_len, emb_dim)\n",
    "        self.pos_decoder = nn.Embedding(max_summary_len, emb_dim)\n",
    "        self.transformer = nn.Transformer(\n",
    "            d_model=emb_dim,\n",
    "            nhead=nhead,\n",
    "            num_encoder_layers=num_layers,\n",
    "            num_decoder_layers=num_layers,\n",
    "            dim_feedforward=ff_dim,\n",
    "            dropout=0.1,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.fc_out = nn.Linear(emb_dim, vocab_size)\n",
    "        self.max_article_len = max_article_len\n",
    "        self.max_summary_len = max_summary_len\n",
    "\n",
    "    def forward(self, src, tgt):\n",
    "        src_mask = None\n",
    "        tgt_mask = nn.Transformer.generate_square_subsequent_mask(tgt.size(1)).to(tgt.device)\n",
    "        src_pos = self.pos_encoder(torch.arange(self.max_article_len, device=src.device)).unsqueeze(0)\n",
    "        tgt_pos = self.pos_decoder(torch.arange(self.max_summary_len, device=tgt.device)).unsqueeze(0)\n",
    "        \n",
    "        src_emb = self.embedding(src) + src_pos[:, :src.size(1), :]\n",
    "        tgt_emb = self.embedding(tgt) + tgt_pos[:, :tgt.size(1), :]\n",
    "        \n",
    "        outs = self.transformer(src_emb, tgt_emb, src_mask=src_mask, tgt_mask=tgt_mask)\n",
    "        return self.fc_out(outs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a62c0ca4-476f-4dd7-b1d2-61064e1fc079",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "# Instantiate and load model\n",
    "model = TransformerSummarizer(\n",
    "    vocab_size=len(vocab),\n",
    "    emb_dim=EMB_DIM,\n",
    "    nhead=N_HEADS,\n",
    "    ff_dim=FF_DIM,\n",
    "    num_layers=LAYERS,\n",
    "    max_article_len=MAX_ART_LEN,\n",
    "    max_summary_len=MAX_SUM_LEN,\n",
    "    pad_idx=pad_idx\n",
    ").to(device)\n",
    "\n",
    "# Load checkpoint\n",
    "model.load_state_dict(torch.load(CHECKPOINT_PATH, map_location=device))\n",
    "print('Checkpoint loaded successfully!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ae97a972-bc9e-4c68-b328-166c29b1940a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup optimizer, criterion, scheduler\n",
    "optim = torch.optim.Adam(model.parameters(), lr=INIT_LR)\n",
    "sched = CosineAnnealingWarmRestarts(optim, T_0=max(len(train_loader)//WARMUP_EPOCHS,1), T_mult=2, eta_min=1e-6)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=pad_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2f782601-463f-416b-8aac-0bba6b368868",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial val-loss: 4.843511923035579\n"
     ]
    }
   ],
   "source": [
    "# Verify checkpoint loading by checking validation loss\n",
    "def eval_loss():\n",
    "    model.eval()\n",
    "    tot = 0\n",
    "    with torch.no_grad():\n",
    "        for src, tgt in val_loader:\n",
    "            src, tgt = src.to(device), tgt.to(device)\n",
    "            # CONSISTENT with training: use tgt[:, :-1] as input\n",
    "            logits = model(src, tgt[:, :-1])\n",
    "            loss = criterion(logits.reshape(-1, logits.size(-1)), tgt[:, 1:].reshape(-1))\n",
    "            tot += loss.item()\n",
    "    return tot/len(val_loader)\n",
    "\n",
    "print('Initial val-loss:', eval_loss())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fe0a3eb6-1d23-4bb2-a980-f05dfd8ab7cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROUGE-L helper function\n",
    "scorer = rouge_scorer.RougeScorer(['rougeL'], use_stemmer=True)\n",
    "\n",
    "def rougeL_sample(model, n=50):\n",
    "    model.eval()\n",
    "    scores = []\n",
    "    with torch.no_grad():\n",
    "        for i in range(n):\n",
    "            art = val_df['clean_article'].iloc[i].split()[:MAX_ART_LEN]\n",
    "            art_ids = [vocab.get(t, vocab[UNK_TOKEN]) for t in art] + [pad_idx]*(MAX_ART_LEN-len(art))\n",
    "            src = torch.tensor(art_ids, device=device).unsqueeze(0)\n",
    "            \n",
    "            # Simple greedy decode\n",
    "            tgt = torch.tensor([[vocab[START_TOKEN]]], device=device)\n",
    "            for _ in range(MAX_SUM_LEN-1):\n",
    "                logits = model(src, tgt)\n",
    "                next_id = logits[:,-1].argmax(-1, keepdim=True)\n",
    "                tgt = torch.cat([tgt, next_id], 1)\n",
    "                if next_id.item() == vocab[END_TOKEN]:\n",
    "                    break\n",
    "            \n",
    "            # Convert to text (skip START, stop at END or PAD)\n",
    "            pred_ids = tgt[0, 1:].tolist()\n",
    "            if vocab[END_TOKEN] in pred_ids:\n",
    "                pred_ids = pred_ids[:pred_ids.index(vocab[END_TOKEN])]\n",
    "            pred_text = ' '.join([list(vocab.keys())[list(vocab.values()).index(idx)] \n",
    "                                for idx in pred_ids if idx not in [vocab[PAD_TOKEN], vocab[START_TOKEN]]])\n",
    "            \n",
    "            ref_text = val_df['clean_summary'].iloc[i]\n",
    "            scores.append(scorer.score(ref_text, pred_text)['rougeL'].fmeasure)\n",
    "    return sum(scores)/len(scores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ee200a0c-e80f-407e-a640-c9e4b2596a89",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████████████████████████████████████████████████████████████████| 1515/1515 [08:59<00:00,  2.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: train 4.273 | val 4.850 | tf_prob 1.00\n",
      "New best model saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|██████████████████████████████████████████████████████████████████████| 1515/1515 [09:07<00:00,  2.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: train 4.533 | val 4.871 | tf_prob 0.90\n",
      "  No improvement (patience 1/5)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|██████████████████████████████████████████████████████████████████████| 1515/1515 [09:12<00:00,  2.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: train 4.524 | val 4.876 | tf_prob 0.81\n",
      "  No improvement (patience 2/5)\n",
      "  ROUGE-L (50 samples): 0.083\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4:   1%|▍                                                                        | 8/1515 [00:03<10:31,  2.39it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 18\u001b[39m\n\u001b[32m     15\u001b[39m decoder_in = tgt[:, :-\u001b[32m1\u001b[39m].clone()  \u001b[38;5;66;03m# Ground truth input\u001b[39;00m\n\u001b[32m     16\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m     17\u001b[39m     \u001b[38;5;66;03m# Get model predictions for previous time steps\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m18\u001b[39m     pred_ids = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43msrc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecoder_in\u001b[49m\u001b[43m)\u001b[49m.argmax(-\u001b[32m1\u001b[39m)\n\u001b[32m     20\u001b[39m \u001b[38;5;66;03m# Apply scheduled sampling mask\u001b[39;00m\n\u001b[32m     21\u001b[39m mask = (torch.rand_like(decoder_in.float()) > tf_prob) & (decoder_in != pad_idx)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/mnt/d/ML/Projects/ai-powered-research-assistant/assistant-gpu/lib/python3.13/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/mnt/d/ML/Projects/ai-powered-research-assistant/assistant-gpu/lib/python3.13/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 30\u001b[39m, in \u001b[36mTransformerSummarizer.forward\u001b[39m\u001b[34m(self, src, tgt)\u001b[39m\n\u001b[32m     27\u001b[39m src_emb = \u001b[38;5;28mself\u001b[39m.embedding(src) + src_pos[:, :src.size(\u001b[32m1\u001b[39m), :]\n\u001b[32m     28\u001b[39m tgt_emb = \u001b[38;5;28mself\u001b[39m.embedding(tgt) + tgt_pos[:, :tgt.size(\u001b[32m1\u001b[39m), :]\n\u001b[32m---> \u001b[39m\u001b[32m30\u001b[39m outs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtransformer\u001b[49m\u001b[43m(\u001b[49m\u001b[43msrc_emb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtgt_emb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msrc_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43msrc_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtgt_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtgt_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     31\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.fc_out(outs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/mnt/d/ML/Projects/ai-powered-research-assistant/assistant-gpu/lib/python3.13/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/mnt/d/ML/Projects/ai-powered-research-assistant/assistant-gpu/lib/python3.13/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/mnt/d/ML/Projects/ai-powered-research-assistant/assistant-gpu/lib/python3.13/site-packages/torch/nn/modules/transformer.py:276\u001b[39m, in \u001b[36mTransformer.forward\u001b[39m\u001b[34m(self, src, tgt, src_mask, tgt_mask, memory_mask, src_key_padding_mask, tgt_key_padding_mask, memory_key_padding_mask, src_is_causal, tgt_is_causal, memory_is_causal)\u001b[39m\n\u001b[32m    266\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m    267\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mthe feature number of src and tgt must be equal to d_model\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    268\u001b[39m     )\n\u001b[32m    270\u001b[39m memory = \u001b[38;5;28mself\u001b[39m.encoder(\n\u001b[32m    271\u001b[39m     src,\n\u001b[32m    272\u001b[39m     mask=src_mask,\n\u001b[32m    273\u001b[39m     src_key_padding_mask=src_key_padding_mask,\n\u001b[32m    274\u001b[39m     is_causal=src_is_causal,\n\u001b[32m    275\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m276\u001b[39m output = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdecoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    277\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtgt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    278\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmemory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    279\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtgt_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtgt_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    280\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmemory_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmemory_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    281\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtgt_key_padding_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtgt_key_padding_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    282\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmemory_key_padding_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmemory_key_padding_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    283\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtgt_is_causal\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtgt_is_causal\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    284\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmemory_is_causal\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmemory_is_causal\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    285\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    286\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m output\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/mnt/d/ML/Projects/ai-powered-research-assistant/assistant-gpu/lib/python3.13/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/mnt/d/ML/Projects/ai-powered-research-assistant/assistant-gpu/lib/python3.13/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/mnt/d/ML/Projects/ai-powered-research-assistant/assistant-gpu/lib/python3.13/site-packages/torch/nn/modules/transformer.py:607\u001b[39m, in \u001b[36mTransformerDecoder.forward\u001b[39m\u001b[34m(self, tgt, memory, tgt_mask, memory_mask, tgt_key_padding_mask, memory_key_padding_mask, tgt_is_causal, memory_is_causal)\u001b[39m\n\u001b[32m    604\u001b[39m output = tgt\n\u001b[32m    606\u001b[39m seq_len = _get_seq_len(tgt, \u001b[38;5;28mself\u001b[39m.layers[\u001b[32m0\u001b[39m].self_attn.batch_first)\n\u001b[32m--> \u001b[39m\u001b[32m607\u001b[39m tgt_is_causal = \u001b[43m_detect_is_causal_mask\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtgt_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtgt_is_causal\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseq_len\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    609\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m mod \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.layers:\n\u001b[32m    610\u001b[39m     output = mod(\n\u001b[32m    611\u001b[39m         output,\n\u001b[32m    612\u001b[39m         memory,\n\u001b[32m   (...)\u001b[39m\u001b[32m    618\u001b[39m         memory_is_causal=memory_is_causal,\n\u001b[32m    619\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/mnt/d/ML/Projects/ai-powered-research-assistant/assistant-gpu/lib/python3.13/site-packages/torch/nn/modules/transformer.py:1206\u001b[39m, in \u001b[36m_detect_is_causal_mask\u001b[39m\u001b[34m(mask, is_causal, size)\u001b[39m\n\u001b[32m   1203\u001b[39m \u001b[38;5;66;03m# Do not use `torch.equal` so we handle batched masks by\u001b[39;00m\n\u001b[32m   1204\u001b[39m \u001b[38;5;66;03m# broadcasting the comparison.\u001b[39;00m\n\u001b[32m   1205\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m mask.size() == causal_comparison.size():\n\u001b[32m-> \u001b[39m\u001b[32m1206\u001b[39m     make_causal = \u001b[38;5;28;43mbool\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmask\u001b[49m\u001b[43m \u001b[49m\u001b[43m==\u001b[49m\u001b[43m \u001b[49m\u001b[43mcausal_comparison\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mall\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1207\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1208\u001b[39m     make_causal = \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# Fine-tuning Loop with Scheduled Sampling\n",
    "best_val = float('inf')\n",
    "patience_counter = 0\n",
    "\n",
    "for epoch in range(1, EXTRA_EPOCHS+1):\n",
    "    tf_prob = TEACHER_P0 * (TEACHER_DECAY ** (epoch-1))\n",
    "    model.train()\n",
    "    tot = 0\n",
    "    \n",
    "    for src, tgt in tqdm(train_loader, desc=f'Epoch {epoch}'):\n",
    "        src, tgt = src.to(device), tgt.to(device)\n",
    "        optim.zero_grad()\n",
    "\n",
    "        # Scheduled sampling input build\n",
    "        decoder_in = tgt[:, :-1].clone()  # Ground truth input\n",
    "        with torch.no_grad():\n",
    "            # Get model predictions for previous time steps\n",
    "            pred_ids = model(src, decoder_in).argmax(-1)\n",
    "        \n",
    "        # Apply scheduled sampling mask\n",
    "        mask = (torch.rand_like(decoder_in.float()) > tf_prob) & (decoder_in != pad_idx)\n",
    "        decoder_in[mask] = pred_ids[mask]\n",
    "\n",
    "        logits = model(src, decoder_in)\n",
    "        loss = criterion(logits.reshape(-1, logits.size(-1)), tgt[:, 1:].reshape(-1))\n",
    "        loss.backward()\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), CLIP_NORM)\n",
    "        optim.step()\n",
    "        sched.step()\n",
    "        tot += loss.item()\n",
    "\n",
    "    train_loss = tot/len(train_loader)\n",
    "    val_loss = eval_loss()\n",
    "    \n",
    "    print(f'Epoch {epoch}: train {train_loss:.3f} | val {val_loss:.3f} | tf_prob {tf_prob:.2f}')\n",
    "\n",
    "    if val_loss < best_val - 1e-4:\n",
    "        best_val = val_loss\n",
    "        patience_counter = 0\n",
    "        torch.save(model.state_dict(), CHECKPOINT_PATH)\n",
    "        print('New best model saved.')\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        print(f'  No improvement (patience {patience_counter}/{PATIENCE})')\n",
    "        if patience_counter >= PATIENCE:\n",
    "            print('Early stopping.')\n",
    "            break\n",
    "\n",
    "    # ROUGE-L sample check every 3 epochs\n",
    "    if epoch % 3 == 0:\n",
    "        rouge_score = rougeL_sample(model)\n",
    "        print(f'  ROUGE-L (50 samples): {rouge_score:.3f}')\n",
    "\n",
    "print(f'Fine-tuning complete. Best validation loss: {best_val:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8afcc427-29b8-4777-8ccc-bba332d0b564",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
