{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f3516b0b-4a4e-491d-8b95-1efd39e1c582",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "da73bf8a-05ca-4e6d-b9c6-8f15e62da2f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "vocab_path = '../../models/sentiment/vocab.json'\n",
    "save_path = '../../models/sentiment/'\n",
    "train_df = pd.read_csv('../../data/sentiment-analysis/processed_train.csv')\n",
    "val_df = pd.read_csv('../../data/sentiment-analysis/processed_val.csv')\n",
    "\n",
    "# Prep text column safety\n",
    "train_df = train_df.dropna(subset=['clean_text'])\n",
    "train_df['clean_text'] = train_df['clean_text'].astype(str)\n",
    "val_df = val_df.dropna(subset=['clean_text'])\n",
    "val_df['clean_text'] = val_df['clean_text'].astype(str)\n",
    "\n",
    "max_len = 100\n",
    "embedding_dim = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "90458d35-2073-458a-af35-f8ac0020418f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab saved.\n",
      "Vocab size: 9298\n"
     ]
    }
   ],
   "source": [
    "# Build Vocabulary from training data (simple word-to-index)\n",
    "def build_vocab(texts, min_freq=2):\n",
    "    from collections import Counter\n",
    "    counter = Counter()\n",
    "    for text in texts:\n",
    "        tokens = text.split()\n",
    "        counter.update(tokens)\n",
    "    vocab = [w for w, freq in counter.items() if freq >= min_freq]\n",
    "    word2idx = {w: i + 2 for i, w in enumerate(vocab)}\n",
    "    word2idx['<PAD>'] = 0\n",
    "    word2idx['<UNK>'] = 1\n",
    "    return word2idx\n",
    "\n",
    "vocab = build_vocab(train_df['clean_text'])\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "# Save\n",
    "import json\n",
    "with open(vocab_path, 'w') as f:\n",
    "    json.dump(vocab, f)\n",
    "\n",
    "print(\"Vocab saved.\")\n",
    "print(\"Vocab size:\", vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "15b37db3-13c2-48d5-822e-5c4d8de2c15c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset class\n",
    "class SentimentDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, df, word2idx, max_len=max_len):\n",
    "        self.texts = df['clean_text'].values\n",
    "        self.labels = df['label'].values\n",
    "        self.word2idx = word2idx\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def encode(self, text):\n",
    "        tokens = text.split()\n",
    "        ids = [self.word2idx.get(t, self.word2idx['<UNK>']) for t in tokens]\n",
    "        if len(ids) < self.max_len:\n",
    "            ids.extend([self.word2idx['<PAD>']] * (self.max_len - len(ids)))\n",
    "        else:\n",
    "            ids = ids[:self.max_len]\n",
    "        return ids\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = torch.tensor(self.encode(self.texts[idx]), dtype=torch.long)\n",
    "        y = torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3eb682d0-aa11-4010-ae19-830261615d2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataLoaders\n",
    "train_dataset = SentimentDataset(train_df, vocab)\n",
    "val_dataset = SentimentDataset(val_df, vocab)\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e87920a0-e2db-450b-96c6-b479b4672580",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTM classifier\n",
    "class LSTMClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim=128, hidden_dim=128, output_dim=3, num_layers=2, dropout=0.3):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers, batch_first=True, dropout=dropout)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        embedded = self.embedding(x)\n",
    "        lstm_out, (hidden, _) = self.lstm(embedded)\n",
    "        out = self.dropout(hidden[-1])\n",
    "        out = self.fc(out)\n",
    "        return out\n",
    "\n",
    "# Bi-LSTM\n",
    "class BiLSTMClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim=embedding_dim, hidden_dim=128, output_dim=3, num_layers=2, dropout=0.3):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers, batch_first=True, dropout=dropout, bidirectional=True)\n",
    "        self.fc = nn.Linear(hidden_dim * 2, output_dim)  # 2 for bidirectional\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        embedded = self.embedding(x)\n",
    "        lstm_out, (hidden, _) = self.lstm(embedded)\n",
    "        hidden_cat = torch.cat((hidden[-2], hidden[-1]), dim=1)\n",
    "        out = self.dropout(hidden_cat)\n",
    "        return self.fc(out)\n",
    "\n",
    "# Instantiate the model\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = LSTMClassifier(vocab_size).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6162fd08-1f9c-4783-a559-b12dbcfbf986",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training setup\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = BiLSTMClassifier(vocab_size).to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "68415d19-4d23-4d5c-a778-603e0cb2fbf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train acc 0.586, Val acc 0.665\n",
      "Epoch 2: Train acc 0.720, Val acc 0.699\n",
      "Epoch 3: Train acc 0.785, Val acc 0.707\n",
      "Epoch 4: Train acc 0.835, Val acc 0.701\n",
      "Epoch 5: Train acc 0.882, Val acc 0.699\n",
      "Epoch 6: Train acc 0.914, Val acc 0.683\n",
      "Epoch 7: Train acc 0.939, Val acc 0.691\n",
      "Finished training. Best val acc: 0.7074235807860262\n"
     ]
    }
   ],
   "source": [
    "# Training loop with best model save\n",
    "best_val_acc = 0.0\n",
    "\n",
    "for epoch in range(7):\n",
    "    model.train()\n",
    "    total_loss, correct, total = 0, 0, 0\n",
    "    for x_batch, y_batch in train_loader:\n",
    "        x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(x_batch)\n",
    "        loss = criterion(output, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item() * x_batch.size(0)\n",
    "        preds = output.argmax(dim=1)\n",
    "        correct += (preds == y_batch).sum().item()\n",
    "        total += y_batch.size(0)\n",
    "    train_acc = correct / total\n",
    "\n",
    "    model.eval()\n",
    "    val_correct, val_total = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for x_batch, y_batch in val_loader:\n",
    "            x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n",
    "            output = model(x_batch)\n",
    "            preds = output.argmax(dim=1)\n",
    "            val_correct += (preds == y_batch).sum().item()\n",
    "            val_total += y_batch.size(0)\n",
    "    val_acc = val_correct / val_total\n",
    "\n",
    "    print(f\"Epoch {epoch+1}: Train acc {train_acc:.3f}, Val acc {val_acc:.3f}\")\n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        if not os.path.exists(save_path):\n",
    "            os.makedirs(save_path)\n",
    "        torch.save(model.state_dict(), os.path.join(save_path, 'best_sentiment_model.pt'))\n",
    "\n",
    "print('Finished training. Best val acc:', best_val_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26230163-0bd3-448c-9953-c1c6288ecb5f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
