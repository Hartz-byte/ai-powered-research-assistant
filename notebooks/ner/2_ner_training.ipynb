{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b1d893e2-72a5-4c8e-9635-09c0e0e75c27",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Imports\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a5dc80df-fd42-4f1b-a2c4-130b24e8ef3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load processed data\n",
    "DATA_DIR = '../../data/ner/'\n",
    "Xw_train = np.load(os.path.join(DATA_DIR, 'Xw_train.npy'))\n",
    "Xw_val = np.load(os.path.join(DATA_DIR, 'Xw_val.npy'))\n",
    "Xp_train = np.load(os.path.join(DATA_DIR, 'Xp_train.npy'))\n",
    "Xp_val = np.load(os.path.join(DATA_DIR, 'Xp_val.npy'))\n",
    "Yt_train = np.load(os.path.join(DATA_DIR, 'Yt_train.npy'))\n",
    "Yt_val = np.load(os.path.join(DATA_DIR, 'Yt_val.npy'))\n",
    "\n",
    "with open(os.path.join(DATA_DIR, 'word2idx.pkl'), 'rb') as f:\n",
    "    word2idx = pickle.load(f)\n",
    "with open(os.path.join(DATA_DIR, 'pos2idx.pkl'), 'rb') as f:\n",
    "    pos2idx = pickle.load(f)\n",
    "with open(os.path.join(DATA_DIR, 'tag2idx.pkl'), 'rb') as f:\n",
    "    tag2idx = pickle.load(f)\n",
    "idx2tag = {i: t for t, i in tag2idx.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e359185c-ee2b-463f-820a-9d84658c2e79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset\n",
    "class NERDataset(Dataset):\n",
    "    def __init__(self, Xw, Xp, Yt):\n",
    "        self.Xw = torch.LongTensor(Xw)\n",
    "        self.Xp = torch.LongTensor(Xp)\n",
    "        self.Yt = torch.LongTensor(Yt)\n",
    "    def __len__(self):\n",
    "        return len(self.Xw)\n",
    "    def __getitem__(self, idx):\n",
    "        return self.Xw[idx], self.Xp[idx], self.Yt[idx]\n",
    "\n",
    "train_ds = NERDataset(Xw_train, Xp_train, Yt_train)\n",
    "val_ds = NERDataset(Xw_val, Xp_val, Yt_val)\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "train_dl = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_dl = DataLoader(val_ds, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f678da78-30bc-4c72-afcf-e5091ef5ea78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Definition\n",
    "class BiLSTM_NER(nn.Module):\n",
    "    def __init__(self, vocab_size, pos_size, tag_size, emb_dim=100, pos_emb_dim=16, lstm_units=128):\n",
    "        super().__init__()\n",
    "        self.word_emb = nn.Embedding(vocab_size, emb_dim, padding_idx=0)\n",
    "        self.pos_emb = nn.Embedding(pos_size, pos_emb_dim, padding_idx=0)\n",
    "        self.lstm = nn.LSTM(emb_dim+pos_emb_dim, lstm_units, batch_first=True, bidirectional=True)\n",
    "        self.fc = nn.Linear(lstm_units*2, tag_size)\n",
    "    def forward(self, Xw, Xp):\n",
    "        w = self.word_emb(Xw)\n",
    "        p = self.pos_emb(Xp)\n",
    "        feats = torch.cat([w, p], dim=-1)\n",
    "        lstm_out, _ = self.lstm(feats)\n",
    "        out = self.fc(lstm_out)\n",
    "        return out  # [batch, seq, tag_size]\n",
    "\n",
    "VOCAB_SIZE = len(word2idx)\n",
    "POS_SIZE = len(pos2idx)\n",
    "TAG_SIZE = len(tag2idx)\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "model = BiLSTM_NER(VOCAB_SIZE, POS_SIZE, TAG_SIZE).to(DEVICE)\n",
    "loss_fn = nn.CrossEntropyLoss(ignore_index=0)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "48153231-5b10-407e-83c2-6a9ea5605b1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10: 100%|████████████████████| 600/600 [00:06<00:00, 94.37it/s, loss=0.298]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Valid Loss: 0.1427\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/10: 100%|███████████████████| 600/600 [00:04<00:00, 122.38it/s, loss=0.123]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Valid Loss: 0.1140\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/10: 100%|██████████████████| 600/600 [00:03<00:00, 158.83it/s, loss=0.0994]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Valid Loss: 0.1045\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/10: 100%|███████████████████| 600/600 [00:05<00:00, 117.02it/s, loss=0.079]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Valid Loss: 0.0994\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/10: 100%|██████████████████| 600/600 [00:05<00:00, 107.31it/s, loss=0.0673]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 Valid Loss: 0.1019\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/10: 100%|██████████████████| 600/600 [00:05<00:00, 109.96it/s, loss=0.0566]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 Valid Loss: 0.0987\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/10: 100%|██████████████████| 600/600 [00:05<00:00, 115.75it/s, loss=0.0465]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7 Valid Loss: 0.1035\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/10: 100%|██████████████████| 600/600 [00:04<00:00, 126.92it/s, loss=0.0375]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8 Valid Loss: 0.1091\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/10: 100%|██████████████████| 600/600 [00:04<00:00, 134.59it/s, loss=0.0301]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9 Valid Loss: 0.1163\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/10: 100%|█████████████████| 600/600 [00:04<00:00, 127.35it/s, loss=0.0231]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10 Valid Loss: 0.1238\n"
     ]
    }
   ],
   "source": [
    "# Train Loop\n",
    "EPOCHS = 10\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    progress = tqdm(train_dl, desc=f\"Epoch {epoch+1}/{EPOCHS}\")\n",
    "    for Xw, Xp, Yt in progress:\n",
    "        Xw, Xp, Yt = Xw.to(DEVICE), Xp.to(DEVICE), Yt.to(DEVICE)\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(Xw, Xp)\n",
    "        loss = loss_fn(logits.view(-1, TAG_SIZE), Yt.view(-1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "        progress.set_postfix(loss=total_loss / (progress.n+1))\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    val_loss, val_steps = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for Xw, Xp, Yt in val_dl:\n",
    "            Xw, Xp, Yt = Xw.to(DEVICE), Xp.to(DEVICE), Yt.to(DEVICE)\n",
    "            logits = model(Xw, Xp)\n",
    "            loss = loss_fn(logits.view(-1, TAG_SIZE), Yt.view(-1))\n",
    "            val_loss += loss.item()\n",
    "            val_steps += 1\n",
    "    print(f\"Epoch {epoch+1} Valid Loss: {val_loss / val_steps:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d945e1a3-4628-46fd-85db-80dbfe66980a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved as best_ner_model.pt\n"
     ]
    }
   ],
   "source": [
    "# Save Model\n",
    "SAVE_DIR = '../../models/ner/'\n",
    "torch.save(model.state_dict(), os.path.join(SAVE_DIR, \"best_ner_model.pt\"))\n",
    "print(\"Model saved as best_ner_model.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "382b11bd-2b3a-4f80-a19d-051784d23c76",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
