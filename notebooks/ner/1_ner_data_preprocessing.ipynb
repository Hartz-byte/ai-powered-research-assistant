{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4e386658-f02d-4095-8490-fd84ba31b336",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "97eb19b9-7179-4669-86c1-7d6275efbec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths\n",
    "DATA_DIR = '../../data/ner/'\n",
    "DATA_FILE = os.path.join(DATA_DIR, 'NER_dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "086a9e29-2657-41d1-9204-083a93e73734",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_979/2194672397.py:5: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  df['Sentence #'] = df['Sentence #'].fillna(method='ffill')\n"
     ]
    }
   ],
   "source": [
    "# Load dataset\n",
    "df = pd.read_csv(DATA_FILE, encoding='latin1')\n",
    "\n",
    "# Fill missing sentence numbers and convert to int\n",
    "df['Sentence #'] = df['Sentence #'].fillna(method='ffill')\n",
    "df['Sentence #'] = df['Sentence #'].apply(lambda x: int(str(x).replace('Sentence: ', '')))\n",
    "\n",
    "# Clean and filter data\n",
    "df['Word'] = df['Word'].astype(str).str.strip()\n",
    "df['POS'] = df['POS'].astype(str).str.strip()\n",
    "df['Tag'] = df['Tag'].astype(str).str.strip()\n",
    "df = df[df['Word'] != '']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d5655127-667a-4543-b145-a545d46700da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total sentences: 47959\n",
      "Example sentence: ['Thousands', 'of', 'demonstrators', 'have', 'marched', 'through', 'London', 'to', 'protest', 'the', 'war', 'in', 'Iraq', 'and', 'demand', 'the', 'withdrawal', 'of', 'British', 'troops', 'from', 'that', 'country', '.']\n",
      "Example tags: ['O', 'O', 'O', 'O', 'O', 'O', 'B-geo', 'O', 'O', 'O', 'O', 'O', 'B-geo', 'O', 'O', 'O', 'O', 'O', 'B-gpe', 'O', 'O', 'O', 'O', 'O']\n"
     ]
    }
   ],
   "source": [
    "# Group tokens by sentence\n",
    "grouped = df.groupby('Sentence #')\n",
    "sentences = grouped['Word'].apply(list).tolist()\n",
    "pos_tags = grouped['POS'].apply(list).tolist()\n",
    "ner_tags = grouped['Tag'].apply(list).tolist()\n",
    "\n",
    "print(f\"Total sentences: {len(sentences)}\")\n",
    "print(f\"Example sentence: {sentences[0]}\")\n",
    "print(f\"Example tags: {ner_tags[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "09037529-6d6f-4d08-8c64-619b2ee62396",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab sizes - Words: 35179, POS: 43, Tags: 18\n"
     ]
    }
   ],
   "source": [
    "# Build vocabularies with special tokens\n",
    "def build_vocab(sequences, specials=['<PAD>', '<UNK>']):\n",
    "    vocab = specials.copy()\n",
    "    counter = Counter(tok for seq in sequences for tok in seq)\n",
    "    vocab.extend([w for w, _ in counter.most_common()])\n",
    "    return {w:i for i,w in enumerate(vocab)}\n",
    "\n",
    "word2idx = build_vocab(sentences)\n",
    "pos2idx = build_vocab(pos_tags, specials=['<PAD>'])\n",
    "tag2idx = build_vocab(ner_tags, specials=['<PAD>'])\n",
    "\n",
    "print(f\"Vocab sizes - Words: {len(word2idx)}, POS: {len(pos2idx)}, Tags: {len(tag2idx)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5f74424b-3d01-4986-9ca9-c9bca20e4150",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using max sequence length: 35\n"
     ]
    }
   ],
   "source": [
    "# Determine max sequence length (95th percentile)\n",
    "lengths = [len(s) for s in sentences]\n",
    "max_len = int(np.percentile(lengths, 95))\n",
    "print(f\"Using max sequence length: {max_len}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c0b739d2-de1b-4e35-9e9c-a86ab883c86e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode and pad sequences\n",
    "def encode_and_pad(sequences, vocab, max_len):\n",
    "    encoded = []\n",
    "    for seq in sequences:\n",
    "        enc = [vocab.get(w, vocab.get('<UNK>')) for w in seq]\n",
    "        if len(enc) > max_len: enc = enc[:max_len]\n",
    "        else: enc += [vocab.get('<PAD>')] * (max_len - len(enc))\n",
    "        encoded.append(enc)\n",
    "    return np.array(encoded)\n",
    "\n",
    "X_words = encode_and_pad(sentences, word2idx, max_len)\n",
    "X_pos = encode_and_pad(pos_tags, pos2idx, max_len)\n",
    "Y_tags = encode_and_pad(ner_tags, tag2idx, max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f2f06e0a-7060-43c0-a95f-f000d53466fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 38367, Val size: 9592\n"
     ]
    }
   ],
   "source": [
    "# Train/val split\n",
    "Xw_train, Xw_val, Xp_train, Xp_val, Yt_train, Yt_val = train_test_split(\n",
    "    X_words, X_pos, Y_tags, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "print(f\"Train size: {len(Xw_train)}, Val size: {len(Xw_val)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8dd8e44c-bfa5-4f68-bd81-f3bc5468b761",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data preprocessing completed and saved.\n"
     ]
    }
   ],
   "source": [
    "# Save processed data and vocabs\n",
    "os.makedirs(DATA_DIR, exist_ok=True)\n",
    "\n",
    "np.save(os.path.join(DATA_DIR, 'Xw_train.npy'), Xw_train)\n",
    "np.save(os.path.join(DATA_DIR, 'Xw_val.npy'), Xw_val)\n",
    "np.save(os.path.join(DATA_DIR, 'Xp_train.npy'), Xp_train)\n",
    "np.save(os.path.join(DATA_DIR, 'Xp_val.npy'), Xp_val)\n",
    "np.save(os.path.join(DATA_DIR, 'Yt_train.npy'), Yt_train)\n",
    "np.save(os.path.join(DATA_DIR, 'Yt_val.npy'), Yt_val)\n",
    "\n",
    "with open(os.path.join(DATA_DIR, 'word2idx.pkl'), 'wb') as f:\n",
    "    pickle.dump(word2idx, f)\n",
    "\n",
    "with open(os.path.join(DATA_DIR, 'pos2idx.pkl'), 'wb') as f:\n",
    "    pickle.dump(pos2idx, f)\n",
    "\n",
    "with open(os.path.join(DATA_DIR, 'tag2idx.pkl'), 'wb') as f:\n",
    "    pickle.dump(tag2idx, f)\n",
    "\n",
    "print(\"Data preprocessing completed and saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "954641b7-62f2-4888-aab2-1635682a3edc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
